{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model Repsonses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = '../../../Results/LLMs_Responses'\n",
    "\n",
    "files_list = os.listdir(models_dir)\n",
    "\n",
    "for num, file in enumerate(files_list, start=0):\n",
    "    print(num, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name = 'gpt-4o-last_5_topics-ZS_with_citing_prompting-response.csv'\n",
    "file_name = [file for file in files_list if model_file_name in file][0]\n",
    "\n",
    "responses_file_path = os.path.join(models_dir, file_name)\n",
    "\n",
    "responses_df = pd.read_csv(responses_file_path, encoding='utf-8')\n",
    "responses_df['Problem'] = responses_df['Problem'].apply(eval)\n",
    "\n",
    "print(file_name,'\\n')\n",
    "pd.set_option('display.max_columns', None)\n",
    "responses_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to compare with a similar model, or same model with different configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second_file_index = 17\n",
    "# second_file_name = files_list[second_file_index]\n",
    "\n",
    "# responses_file_path_2 = os.path.join(models_dir, second_file_name)\n",
    "\n",
    "# pd.set_option('display.max_rows', None)\n",
    "\n",
    "# responses_df_2 = pd.read_csv(responses_file_path_2, encoding='utf-8')\n",
    "\n",
    "# diff_df = abs(responses_df_2.loc[:, 'Q1':'Q15'] - responses_df.loc[:, 'Q1':'Q15'])\n",
    "# huge_difference = diff_df.gt(1).sum()  # Adjusted indexing here\n",
    "# diff_df.head()\n",
    "# huge_difference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_none = (responses_df.isna() | (responses_df == '')).sum()\n",
    "columns_with_none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_columns', None)\n",
    "\n",
    "rows_with_none = responses_df[responses_df.iloc[:,:-1].isnull().any(axis=1)]\n",
    "print('Rows with NaN values:', rows_with_none.index.tolist())\n",
    "rows_with_none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display individual response\n",
    "cell_index = 50\n",
    "response_number = 9\n",
    "\n",
    "text = responses_df.at[cell_index, f'Response_{response_number}']\n",
    "\n",
    "display(HTML(\"<div style='white-space: pre-wrap;'>{}</div>\".format(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_with_problems = responses_df[responses_df['Problem'].apply(lambda x: len(x) > 0)].index.tolist()\n",
    "print(indices_with_problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if indices_with_problems:\n",
    "    index_with_problem = 36\n",
    "    responses_with_problem_list = list(responses_df.loc[index_with_problem, 'Problem'])\n",
    "    print(responses_with_problem_list)\n",
    "\n",
    "    response_with_problem = responses_with_problem_list[0]\n",
    "    text = responses_df.loc[index_with_problem, f'Response_{response_with_problem}']\n",
    "    display(HTML(\"<div style='white-space: pre-wrap;'>{}</div>\".format(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display responses for a selected video\n",
    "index_to_display = 207\n",
    "for question_num in range(1, 16):\n",
    "    text = responses_df.loc[index_to_display,f'Response_{question_num}']\n",
    "\n",
    "    print(f'Q{question_num}:', end=' ')\n",
    "    display(HTML(\"<div style='white-space: pre-wrap;'>{}</div>\".format(text)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove prompts if not removed already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_prompt_from_response(response: str) -> str:\n",
    "    phrase = 'Score:'\n",
    "    idx = response.find(phrase)  # Find the starting index of the phrase in the response\n",
    "    if idx != -1:\n",
    "        # Remove the prompt by slicing the response from the end of the phrase\n",
    "        response = response[idx + len(phrase):]\n",
    "    return response\n",
    "\n",
    "def extract_rating(response: str, rating_scale) -> int:\n",
    "    '''\n",
    "    Extract rating integer from beginning of LLM response.\n",
    "    Note: Prompt should have been removed already from the beginning of the response.\n",
    "\n",
    "    Returns:\n",
    "        int or None: The extracted rating if found, otherwise None.\n",
    "    '''\n",
    "    if rating_scale == 5:\n",
    "        pattern = r'([1-5])'\n",
    "    elif rating_scale == 1:\n",
    "        pattern = r'([0-1])'\n",
    "    else:\n",
    "        raise ValueError('rating_scale should be either 5 or 1.')\n",
    "    match = re.search(pattern, response)  # Search for the first encountered integer\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "for i in range(len(responses_df)):\n",
    "    for j in range(1,16):\n",
    "        text = responses_df.at[i, f'Response_{j}']\n",
    "        after_text = remove_prompt_from_response(text)\n",
    "        responses_df.at[i, f'Response_{j}'] = after_text\n",
    "        responses_df.at[i, f'Q{j}'] = extract_rating(after_text, rating_scale=5)\n",
    "\n",
    "responses_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the updated DataFrame after updating the repsonses and scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_output_file = os.path.join(models_dir, model_file_name)\n",
    "\n",
    "responses_df.to_csv(csv_output_file, index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
