{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from IPython.display import display, HTML\n",
    "# from matplotlib.figure import Figure\n",
    "\n",
    "utils_path = os.path.abspath(os.path.join('..'))\n",
    "sys.path.append(utils_path)\n",
    "\n",
    "from Utils.models_analysis_class import ModelsAnalysis\n",
    "from Utils.statistics_plots_analysis_utils import *\n",
    "\n",
    "EXPERT_BOXPLOT_COLOR = '#E74C3C'\n",
    "MODELS_BOXPLOT_COLOR = '#1F77B4'\n",
    "\n",
    "def display_wrapped(text):\n",
    "    display(HTML(\"<div style='white-space: pre-wrap;'>{}</div>\".format(text)))\n",
    "\n",
    "def questionwise_agreement_heatmap(stat_group_df: pd.DataFrame, figsize=None, title=''):\n",
    "    question_avg = stat_group_df.mean(axis=1).values.reshape(-1, 1).round(2)\n",
    "    model_avg = stat_group_df.mean(axis=0).values.reshape(1, -1).round(2)\n",
    "\n",
    "    if figsize is None:\n",
    "        (m, n) = stat_group_df.shape\n",
    "    else:\n",
    "        (m, n) = figsize\n",
    "\n",
    "    asp = 0.5 * m / float(n)\n",
    "    figw = n\n",
    "    figh = figw * asp\n",
    "    gridspec_kw = {'height_ratios': [m, 1], 'width_ratios': [n, 1]}\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(figw, figh), gridspec_kw=gridspec_kw)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, wspace=0.05, hspace=0.1)\n",
    "\n",
    "    hm_kwargs = dict(vmin=0, vmax=1, cmap='coolwarm', annot=True, fmt='.2f', linewidths=.5, linecolor='black')\n",
    "\n",
    "    # Plot stat_group_df heatmap\n",
    "    sns.heatmap(stat_group_df, ax=axes[0, 0], xticklabels=False, cbar=False, **hm_kwargs)\n",
    "\n",
    "    # Plot q_avg heatmap\n",
    "    sns.heatmap(question_avg, ax=axes[0, 1], yticklabels=False, **hm_kwargs)\n",
    "    axes[0, 1].set_xticklabels(['Question Avg'], rotation=90)\n",
    "\n",
    "    # Plot model_avg heatmap\n",
    "    sns.heatmap(model_avg, ax=axes[1, 0], cbar=False, **hm_kwargs)\n",
    "    axes[1, 0].set_xticklabels(stat_group_df.columns, rotation=90)\n",
    "    axes[1, 0].set_yticklabels(['Model Avg'])\n",
    "\n",
    "    # Hide the empty subplot\n",
    "    axes[1, 1].axis('off')\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Function to plot a single comparison of Zero-shot and Chain-of-thought scores across models\n",
    "def plot_comparison_bars(zs_agreement: pd.DataFrame, gs_agreement: pd.DataFrame, \n",
    "                         analysis_instance, ax=None, figsize=(10, 6), annotate=True, filename=None, dpi=500):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    xticklabels = gs_agreement.columns\n",
    "    x = np.arange(len(xticklabels))  # the label locations\n",
    "    width = 0.35  # the width of the bars\n",
    "\n",
    "    light_blue = '#AEC7E8'\n",
    "    dark_blue = '#1F77B4'\n",
    "\n",
    "    for i, model in enumerate(xticklabels):\n",
    "        zs_original_value = zs_agreement[model].iloc[0]\n",
    "        gs_original_value = gs_agreement[model].iloc[0]\n",
    "        \n",
    "        zs_bar_value = zs_original_value if zs_original_value < 0 else zs_original_value * 5\n",
    "        gs_bar_value = gs_original_value if gs_original_value < 0 else gs_original_value * 5\n",
    "        \n",
    "        bar_zs = ax.bar(x[i] - width/2, zs_bar_value, width, bottom=2, color=light_blue)\n",
    "        bar_gs = ax.bar(x[i] + width/2, gs_bar_value, width, bottom=2, color=dark_blue)\n",
    "\n",
    "        if annotate:\n",
    "            ax.annotate(f'{zs_original_value:.2f}', xy=(x[i] - width/2, zs_bar_value + 2), \n",
    "                        xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\n",
    "            ax.annotate(f'{gs_original_value:.2f}', xy=(x[i] + width/2, gs_bar_value + 2), \n",
    "                        xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\n",
    "        \n",
    "    set_plot_properties(ax, ylabel=analysis_instance.agreement_coef)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(xticklabels, rotation=0)\n",
    "    \n",
    "    y_ticks = ['-2', '-1', '0'] + [str(round(x, 1)) for x in np.arange(0.2, 1.2, 0.2)]\n",
    "    ax.set_yticks(np.arange(len(y_ticks)))\n",
    "    ax.set_yticklabels(y_ticks)\n",
    "    \n",
    "    ax.annotate('â†¯', xy=(0, 0), xytext=(0, 0.22),\n",
    "                fontsize=15, va='top', ha='center', color='black',\n",
    "                xycoords='axes fraction', textcoords='axes fraction')\n",
    "    \n",
    "    ax.legend([bar_zs, bar_gs], ['ZS', 'GS'])\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    if filename:\n",
    "        plt.savefig(filename, format=filename.split('.')[-1], dpi=dpi)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "# Function to plot multiple comparisons of Zero-shot and Chain-of-thought scores across models\n",
    "def plot_multiple_comparisons(zs_qw_agreement, gs_qw_agreement, gs_analysis, ncols=2, filename=None, dpi=500):\n",
    "    num_plots = len(gs_analysis.model_names)\n",
    "\n",
    "    # Calculate number of rows\n",
    "    nrows = int(np.ceil(num_plots / ncols))\n",
    "    \n",
    "    # Adjust figsize based on number of columns and rows\n",
    "    fig_width = 10 * ncols\n",
    "    fig_height = 5 * nrows\n",
    "\n",
    "    _, axs = plt.subplots(nrows, ncols, figsize=(fig_width, fig_height))\n",
    "\n",
    "    # Flatten axs if it's a single row/column grid\n",
    "    if nrows == 1 and ncols == 1:\n",
    "        axs = np.array([[axs]])\n",
    "    else:\n",
    "        axs = axs.flatten()\n",
    "\n",
    "    for i, model_name in enumerate(gs_analysis.model_names):\n",
    "        zs_data = zs_qw_agreement[[model_name]].T\n",
    "        gs_data = gs_qw_agreement[[model_name]].T\n",
    "\n",
    "        ax = axs[i]\n",
    "\n",
    "        plot_comparison_bars(zs_data, gs_data, gs_analysis, ax, annotate=False)\n",
    "\n",
    "        ax.set_title(f'Model: {model_name}')\n",
    "\n",
    "    # Turn off the last subplot if there are unused subplots\n",
    "    if num_plots < nrows * ncols:\n",
    "        for j in range(num_plots, nrows * ncols):\n",
    "            axs[j].axis('off')\n",
    "\n",
    "    if filename:\n",
    "        plt.savefig(filename, format=filename.split('.')[-1], dpi=dpi)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare and Save Expert Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# videos_info_file = '../../Getting_Transcripts/merged_filtered_videos_transcripts.csv'\n",
    "# videos_info_df = pd.read_csv(videos_info_file, usecols=['Video ID'], encoding='utf-8')\n",
    "# original_experts_file = '../../../Videos_and_DISCERN_data/videos_info_and_scores.xlsx'\n",
    "# score_columns_to_read = ['Video ID', 'Topic', 'DISCERN1', 'DISCERN2'] + [f'DISCERN1 Q{i}' for i in range(1, 16)] + [f'DISCERN2 Q{i}' for i in range(1, 16)]\n",
    "# experts_df = pd.read_excel(original_experts_file, usecols=score_columns_to_read)\n",
    "# experts_df = merge_dataframes(videos_info_df, experts_df, experts_df.columns)\n",
    "\n",
    "# experts_df.rename(columns=lambda x: x.replace('DISCERN', 'Expert'), inplace=True)\n",
    "\n",
    "# # Find the indices of rows where any of the Q1 to Q15 columns have non-null values, to fill the NaN with 1\n",
    "# # This is to fill Cluster Headque N/A questions with 1\n",
    "# indices = experts_df[EXPERT1_COLUMNS].notna().any(axis=1)\n",
    "# experts_df.loc[indices, EXPERT1_COLUMNS] = experts_df.loc[indices, EXPERT1_COLUMNS].fillna(1)\n",
    "# # Sum the total after filling Nan with 1\n",
    "# experts_df.loc[indices, 'Expert1'] = experts_df.loc[indices, EXPERT1_COLUMNS].sum(axis=1)\n",
    "\n",
    "# # Calculate the mean of 'Expert1' and 'Expert2' columns where 'Expert2' is not NaN, and round it up\n",
    "# mean_discern = np.where(experts_df['Expert2'].notnull(), \n",
    "#                         experts_df[['Expert1', 'Expert2']].mean(axis=1),\n",
    "#                         experts_df['Expert1'])\n",
    "# experts_df.insert(4, 'Experts_Avg', mean_discern)\n",
    "\n",
    "# experts_df.to_csv('../../../Videos_and_DISCERN_data/filtered_experts_scores.csv', index=False)\n",
    "# print(experts_df.shape)\n",
    "# experts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_info_file = '../../Getting_Transcripts/merged_filtered_videos_transcripts.csv'\n",
    "videos_info_df = pd.read_csv(videos_info_file, usecols=['Video ID'], encoding='utf-8')\n",
    "original_experts_file = '../../../Videos_and_DISCERN_data/videos_info_and_scores.xlsx'\n",
    "score_columns_to_read = ['Video ID', 'Topic', 'DISCERN1', 'DISCERN2'\n",
    "                         ] + [f'DISCERN1 Q{i}' for i in range(1, 16)\n",
    "                              ] + [f'DISCERN2 Q{i}' for i in range(1, 16)]\n",
    "original_experts_df = pd.read_excel(original_experts_file, usecols=score_columns_to_read)\n",
    "print(original_experts_df.shape)\n",
    "print(original_experts_df['Topic'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero-Shot Prompting: 7 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs_model_files_dict = {\n",
    "    'BioMistral': 'BioMistral-response.csv',\n",
    "    'Claude-3 Sonnet': 'claude-3-sonnet-20240229-response.csv',\n",
    "    'Falcon': 'falcon-40b-instruct-response.csv',\n",
    "    'GPT-4 Turbo': 'gpt-4-turbo-response.csv',\n",
    "    'GPT-4o': 'gpt-4o-last_5_topics-ZS_prompting-response.csv',\n",
    "    'Gemini-1.0 Pro': 'gemini-1.0-pro-latest-response.csv',\n",
    "    'KTO Mistral': 'KTO_Mistral_PairRM-response.csv',\n",
    "    'Llama-3 70B': 'Llama-3-70B-Instruct-response.csv',\n",
    "    'Llama-3 8B': 'Llama-3-8B-Instruct-response user-only-prompt.csv',\n",
    "    'Meerkat': 'meerkat-response.csv',\n",
    "    'Mistral': 'Mistral-7B-Instruct-response.csv',\n",
    "    'Mixtral 8x22B': 'Mixtral-8x22B-Instruct-response.csv',\n",
    "    'Mixtral 8x7B': 'Mixtral-8x7B-Instruct-response.csv',\n",
    "    'MultiVerse': 'MultiVerse_70B-response.csv',\n",
    "    'Orca-2': 'Orca-2-do_sample=false-response.csv',\n",
    "    'Phi-3 mini': 'Phi-3-mini-4k-instruct-response.csv',\n",
    "    'Qwen-1.5': 'Qwen1_5-72B-response.csv',\n",
    "    'Rhea': 'Rhea-response.csv',\n",
    "    'Vicuna': 'vicuna-33b-response.csv',\n",
    "    'Yi': 'Yi-34B-response.csv',\n",
    "}\n",
    "\n",
    "figures_dir = '../../../Lancet_Paper/Figures'\n",
    "experts_file = '../../../Videos_and_DISCERN_data/filtered_experts_scores.csv'\n",
    "models_dir = '../LLMs_Responses'\n",
    "topics_keys = ['SB', 'FF', 'CH', 'TF', 'PN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero-shot Prompting: ISA topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zs_model_files_dict = {\n",
    "#     'Claude-3 Sonnet': 'claude-3-sonnet-20240229-diabetes-ZS_prompting-response.csv',\n",
    "#     # 'Gemini-1.0 Pro': 'gemini-1.0-pro-latest-diabetes-ZS_prompting-response.csv',\n",
    "#     # 'Gemini-1.5 Pro': 'gemini-1.5-pro-latest-diabetes-ZS_prompting-response.csv',\n",
    "#     'Gemini-1.5 Flash': 'gemini-1.5-flash-diabetes-ZS_prompting-response.csv',\n",
    "#     'GPT-4o': 'gpt-4o-diabetes-ZS_prompting-response.csv',\n",
    "# }\n",
    "\n",
    "# figures_dir = '../../../ISA_Paper/Figures'\n",
    "# experts_file = '../../../ISA_Paper/Data/diabetes_experts_scores.csv'\n",
    "# models_dir = '../../../ISA_Paper/Data/Results'\n",
    "# topics_keys = ['ISA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero-shot: binary questions: first 2 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zs_model_files_dict = {\n",
    "#     # 'Claude-3 Sonnet': 'claude-3-sonnet-20240229-diabetes-zero_shot_prompting-response.csv',\n",
    "#     'Gemini-1.0 Pro': 'gemini-1.0-pro-first_2_topics-ZS_prompting-binary_questions-response.csv',\n",
    "#     'Gemini-1.5 Pro': 'gemini-1.5-pro-latest-first_2_topics-ZS_prompting-binary_questions-response.csv',\n",
    "#     'GPT-4o': 'gpt-4o-first_2_topics-ZS-binary_questions-response.csv',\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(figures_dir):\n",
    "    os.makedirs(figures_dir)\n",
    "fig_format = 'pdf'\n",
    "fig_dpi = 500\n",
    "\n",
    "categories = [1, 2, 3, 4, 5]\n",
    "agreement_coef = 'Brennan-Prediger Kappa'\n",
    "weights_type = 'quadratic'\n",
    "\n",
    "zs_analysis = ModelsAnalysis(zs_model_files_dict, experts_file, models_dir, topics_keys, \n",
    "                         categories, agreement_coef, weights_type)\n",
    "\n",
    "zs_analysis.load_experts_data()\n",
    "zs_analysis.process_models()\n",
    "\n",
    "print('Data shape:', zs_analysis.group_df.shape)\n",
    "zs_analysis.group_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Score Analysis\n",
    "Total scores out of 75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experts Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total score agreement between Expert 1 and Expert 2:', \n",
    "      round(zs_analysis.calculate_total_experts_agreement(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of Total Experts Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_bins = list(range(15,80,5))\n",
    "fig, _ = create_plot('histplot', zs_analysis.group_df, x='Experts_Avg', bins=total_bins,\n",
    "            xlabel='Experts_Avg Scores', ylabel='Frequency',\n",
    "            figsize=(10,5))\n",
    "\n",
    "# fig_title = \"Frequency distribution of experts' total average scores\"\n",
    "# fig_file_name = os.path.join(figures_dir, fig_title + '.' + fig_format)\n",
    "# fig.savefig(fig_file_name, format=fig_format, dpi=fig_dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of Scores Across Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 5, figsize=(15, 10), sharex=True, sharey=True)\n",
    "\n",
    "qw_bins = [x - 0.5 for x in categories] + [categories[-1] + 0.5]\n",
    "\n",
    "# Flatten the axes array to iterate over it easily\n",
    "axes = axes.flatten()\n",
    "\n",
    "max_y_lim = len(zs_analysis.group_df) + 5\n",
    "# Iterate over each column and create a countplot\n",
    "for i, (expert_column, q_column) in enumerate(zip(EXPERTS_AVG_COLUMNS, QUESTIONS_COLUMNS)):\n",
    "    create_plot('histplot', zs_analysis.group_df, \n",
    "                x=expert_column, ax=axes[i],\n",
    "                title=q_column, xlabel='Score', ylabel='Frequency',\n",
    "                ylim=(0, max_y_lim), bins=qw_bins)\n",
    "\n",
    "# fig.suptitle(\"Distribution of experts' average scores of individual questions\")\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# fig_title = \"Distribution of experts' average scores of individual questions\"\n",
    "# fig_file_name = os.path.join(figures_dir, fig_title + '.' + fig_format)\n",
    "# fig.savefig(fig_file_name, format=fig_format, dpi=fig_dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs_total_expert_models_agreement = zs_analysis.calculate_total_expert_models_agreement(models_in_order=True)\n",
    "\n",
    "zs_descriptive_stat_df = zs_analysis.generate_descriptive_stat()\n",
    "zs_descriptive_stat_df = zs_descriptive_stat_df.iloc[::-1]\n",
    "\n",
    "mean_color = '#1F77B4'  # dark blue\n",
    "std_color = '#FFA07A'   # Light salmon\n",
    "\n",
    "# Plot the horizontal bar chart\n",
    "bars = zs_descriptive_stat_df.plot(kind='barh', figsize=(14, 10), color=[mean_color, std_color])\n",
    "for bar in bars.patches:\n",
    "    width = bar.get_width()\n",
    "    plt.annotate(f'{width:.2f}', xy=(width, bar.get_y() + bar.get_height() / 2),\n",
    "                 xytext=(3, 0), textcoords='offset points', ha='left', va='center')\n",
    "\n",
    "set_plot_properties(bars, xlabel='Values', ylabel='Models', \n",
    "                    )\n",
    "\n",
    "# fig_title = \"Mean and standard deviation of total scores by expert and LLMs for ZS prompting\"\n",
    "# fig_file_name = os.path.join(figures_dir, fig_title + '.' + fig_format)\n",
    "# plt.savefig(fig_file_name, format=fig_format, dpi=fig_dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = {model: MODELS_BOXPLOT_COLOR for model in zs_analysis.model_names}\n",
    "palette['Experts_Avg'] = EXPERT_BOXPLOT_COLOR\n",
    "\n",
    "fig, _ = create_plot('boxplot', data=zs_analysis.group_df[['Experts_Avg'] + zs_analysis.model_names],\n",
    "                     palette=palette, figsize=(12,6), ylim=(10, 80),\n",
    "                     xlabel='Models', ylabel='Scores',\n",
    "                     xticks_rotation=90)\n",
    "\n",
    "# fig_title = \"Box-and-whisker plot of expert and models total scores for ZS prompting\"\n",
    "# fig_file_name = os.path.join(figures_dir, fig_title + '.' + fig_format)\n",
    "# fig.savefig(fig_file_name, format=fig_format, dpi=fig_dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expert-Models Inter-Rater Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, _ = create_plot('bar', zs_total_expert_models_agreement,\n",
    "            ylabel=agreement_coef,\n",
    "            xticks_rotation=90, figsize=(14, 5))\n",
    "\n",
    "# fig_title = \"Expert-Model inter-rater agreement on total scores for ZS prompting\"\n",
    "# fig_file_name = os.path.join(figures_dir, fig_title + '.' + fig_format)\n",
    "# fig.savefig(fig_file_name, format=fig_format, dpi=fig_dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group_df_sorted = zs_analysis.group_df.sort_values(by='Experts_Avg')\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# # Scatter plot for Experts_Avg\n",
    "# ax.scatter(group_df_sorted['Video ID'], group_df_sorted['Experts_Avg'], color='blue', label='Experts_Avg')\n",
    "\n",
    "# models_to_plot = ['Gemini-1.0 Pro']#, 'MultiVerse', 'GPT-4 Turbo']\n",
    "# # Scatter plot for each model\n",
    "# for model in models_to_plot:\n",
    "#     ax.scatter(group_df_sorted['Video ID'], group_df_sorted[model], label=model)\n",
    "        \n",
    "# set_plot_properties(ax, xlabel='Video ID', ylabel='Scores',\n",
    "#                     title='Scatter Plot of Experts_Avg and Models')\n",
    "# plt.legend()\n",
    "# plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question-Wise Analysis\n",
    "Individual 15 scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experts Inter-Rater Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qw_experts_agreement = zs_analysis.calculate_qw_experts_agreement()\n",
    "\n",
    "fig, _ = create_plot('barh', data=qw_experts_agreement,\n",
    "            figsize=(10, 5),\n",
    "            xlabel=agreement_coef, ylabel='Questions',\n",
    "            xticks_rotation=0)\n",
    "\n",
    "# fig_title = \"Expert-Expert question-wise agreement (before removing discrepancy)\"\n",
    "# fig_file_name = os.path.join(figures_dir, fig_title + '.' + fig_format)\n",
    "# fig.savefig(fig_file_name, format=fig_format, dpi=fig_dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expert-Model Inter-Rater Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zs_qw_expert_models_agreement = zs_analysis.calculate_qw_expert_models_agreement()\n",
    "\n",
    "# _ = questionwise_agreement_heatmap(zs_qw_expert_models_agreement, figsize=(14,14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing distant ratings between Expert 1 and Expert 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs_cleaned_group_df, num_of_distant_ratings_per_q = zs_analysis.remove_distant_ratings(\n",
    "    QUESTIONS_COLUMNS, EXPERT1_COLUMNS, EXPERT2_COLUMNS, max_diff=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_notna = zs_cleaned_group_df[EXPERTS_AVG_COLUMNS].notna().sum()\n",
    "\n",
    "# Create a DataFrame with these counts\n",
    "videos_count_df = pd.DataFrame({'Count': count_notna.values}, index=QUESTIONS_COLUMNS)\n",
    "videos_count_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experts Inter-Rater Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qw_experts_agreement = zs_analysis.calculate_qw_experts_agreement(zs_cleaned_group_df)\n",
    "\n",
    "fig, _ = create_plot('barh', data=qw_experts_agreement,\n",
    "            figsize=(10, 5),\n",
    "            xlabel=agreement_coef, ylabel='Questions',\n",
    "            xlim=(-0.32,1),\n",
    "            xticks_rotation=0)\n",
    "\n",
    "# fig_title = \"Expert-Expert question-wise agreement (after removing discrepancy)\"\n",
    "# fig_file_name = os.path.join(figures_dir, fig_title + '.' + fig_format)\n",
    "# fig.savefig(fig_file_name, format=fig_format, dpi=fig_dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expert-Model Inter-Rater Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs_qw_expert_models_agreement = zs_analysis.calculate_qw_expert_models_agreement()\n",
    "\n",
    "title = \"Expert-Model inter-rater agreement on individual question scores for ZS prompting\"\n",
    "fig = questionwise_agreement_heatmap(zs_qw_expert_models_agreement, figsize=(14,14))\n",
    "\n",
    "# fig_file_name = os.path.join(figures_dir, title + '.' + fig_format)\n",
    "# fig.savefig(fig_file_name, format=fig_format, dpi=fig_dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtering_qw_agreement(qw_agreement_df, avg_threshold=0.3):\n",
    "    '''Keeping only models with average agreement above `avg_threshold`'''\n",
    "    model_avg = qw_agreement_df.mean(axis=0).round(2)\n",
    "    model_names_above_threshold = model_avg[model_avg > avg_threshold].index.tolist()\n",
    "    return qw_agreement_df[model_names_above_threshold]\n",
    "\n",
    "# cleaned_qw_agreement_filtered = filtering_qw_agreement(zs_qw_expert_models_agreement)\n",
    "\n",
    "# _ = questionwise_agreement_heatmap(cleaned_qw_agreement_filtered, figsize=(10,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percent of Correct Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percent_of_correct_scores_df = pd.DataFrame(index=QUESTIONS_COLUMNS, columns=zs_analysis.model_names, dtype=int)\n",
    "\n",
    "# diff = 1.5\n",
    "# model_columns = []\n",
    "# for model_name in zs_analysis.model_names:\n",
    "#     for question, expert_col in zip(QUESTIONS_COLUMNS, EXPERTS_AVG_COLUMNS):\n",
    "#         model_col = ' '.join([model_name, question])\n",
    "#         valid_rows = zs_cleaned_group_df[expert_col].notna()\n",
    "        \n",
    "#         diff_series = abs(zs_cleaned_group_df.loc[valid_rows, expert_col] - zs_analysis.group_df.loc[valid_rows, model_col])\n",
    "        \n",
    "#         # Count the number of differences less than 2\n",
    "#         correct_scores_percent = diff_series.lt(diff).sum() / diff_series.count()\n",
    "#         # Store the result in the DataFrame\n",
    "#         percent_of_correct_scores_df.at[question, model_name] = correct_scores_percent\n",
    "\n",
    "# _ = questionwise_agreement_heatmap(percent_of_correct_scores_df, title=f'Percent of differences < {diff}')#, figsize=(14,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models Inter-Rater Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good_threshold = 0.6\n",
    "# bad_threshold = 0.2\n",
    "\n",
    "# good_models = zs_total_expert_models_agreement.columns[zs_total_expert_models_agreement.loc['Brennan-Prediger Kappa'] > good_threshold].tolist()\n",
    "# moderate_models = zs_total_expert_models_agreement.columns[(zs_total_expert_models_agreement.loc['Brennan-Prediger Kappa'] <= good_threshold) \n",
    "#                                         & (zs_total_expert_models_agreement.loc['Brennan-Prediger Kappa'] > bad_threshold)].tolist()\n",
    "# bad_models = zs_total_expert_models_agreement.columns[zs_total_expert_models_agreement.loc['Brennan-Prediger Kappa'] <= bad_threshold].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # models = good_models + moderate_models + bad_models\n",
    "# zs_models_agreement_df = zs_analysis.calculate_models_agreement()\n",
    "\n",
    "# _ = create_plot('heatmap', data=zs_models_agreement_df,\n",
    "#                 figsize=(12,6),\n",
    "#                 title=f'Models Total Score Agreement Heatmap',\n",
    "#                 xticks_rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain-of-Thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_model_files_dict = {\n",
    "    'GPT-4o': 'gpt-4o-last_5_topics-GS_prompting-response.csv',\n",
    "    'Gemini-1.0 Pro': 'gemini-1.0-pro-latest-last_5_topics-GS_prompting-response.csv',\n",
    "    'MultiVerse': 'MultiVerse_70B-last_5_topics-GS_prompting-response.csv',\n",
    "    'GPT-4 Turbo': 'gpt-4-turbo-last_5_topics-GS_prompting-response.csv',\n",
    "    # 'Llama-3 70B': 'Llama-3-70B-Instruct-last_5_topics-GS_prompting-response.csv',\n",
    "    'Claude-3 Sonnet': 'claude-3-sonnet-20240229-last_5_topics-GS_prompting-response.csv',\n",
    "    'Orca-2': 'Orca-2-last_5_topics-GS_prompting-response.csv',\n",
    "    'KTO Mistral': 'KTO_Mistral_PairRM-last_5_topics-GS_prompting-response.csv',\n",
    "    'Phi-3 mini': 'Phi-3-mini-128k-instruct-last_5_topics-GS_prompting-response.csv',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gs_model_files_dict = {\n",
    "#     'Claude-3 Sonnet': 'claude-3-sonnet-20240229-diabetes-GS_prompting-response.csv',\n",
    "#     # 'Gemini-1.0 Pro': 'gemini-1.0-pro-latest-diabetes-GS_prompting-response.csv',\n",
    "#     # 'Gemini-1.5 Pro': 'gemini-1.5-pro-latest-diabetes-GS_prompting-response.csv',\n",
    "#     'Gemini-1.5 Flash': 'gemini-1.5-flash-diabetes-GS_prompting-response.csv',\n",
    "#     'GPT-4o': 'gpt-4o-diabetes-GS_prompting-response.csv',\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chain-of-Thought Prompting: last 3 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_model_names(base_analysis, target_analysis):\n",
    "    \"\"\"\n",
    "    Reorders the model names in target_analysis to match the order of model names in base_analysis.\n",
    "\n",
    "    Args:\n",
    "        base_analysis: Instance of ModelsAnalysis for the order to follow.\n",
    "        target_analysis: Instance of ModelsAnalysis for order to update.\n",
    "    \"\"\"\n",
    "    base_model_names = base_analysis.model_names\n",
    "    target_model_names = [model for model in base_model_names if model in target_analysis.model_names]\n",
    "    target_analysis.model_names = target_model_names\n",
    "\n",
    "gs_analysis = ModelsAnalysis(gs_model_files_dict, experts_file, models_dir, topics_keys,\n",
    "                         categories, agreement_coef, weights_type)\n",
    "\n",
    "gs_analysis.load_experts_data()\n",
    "gs_analysis.process_models()\n",
    "\n",
    "# Reordering models in GS to be same order as ZS, for comparison\n",
    "reorder_model_names(zs_analysis, gs_analysis)\n",
    "\n",
    "gs_total_expert_models_agreement = gs_analysis.calculate_total_expert_models_agreement()\n",
    "\n",
    "gs_cleaned_group_df, num_of_distant_ratings_per_q = gs_analysis.remove_distant_ratings(\n",
    "    QUESTIONS_COLUMNS, EXPERT1_COLUMNS, EXPERT2_COLUMNS, max_diff=1)\n",
    "\n",
    "gs_qw_expert_models_agreement = gs_analysis.calculate_qw_expert_models_agreement(group_df=gs_analysis.group_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_descriptive_stat = gs_analysis.generate_descriptive_stat()\n",
    "gs_descriptive_stat.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = {model: MODELS_BOXPLOT_COLOR for model in gs_analysis.model_names}\n",
    "palette['Experts_Avg'] = EXPERT_BOXPLOT_COLOR\n",
    "\n",
    "_, _ = create_plot('boxplot', data=gs_analysis.group_df[['Experts_Avg'] + gs_analysis.model_names],\n",
    "                   palette=palette, figsize=(10,5), ylim=(10, 80),\n",
    "                   xlabel='Models', ylabel='Scores',\n",
    "                   title='Box plot of expert and models total scores after GS prompting',\n",
    "                   xticks_rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig_title = 'Expert-Model inter-rater agreement on total scores for ZS vs GS prompting'\n",
    "# fig_file_name = os.path.join(figures_dir, fig_title + '.' + fig_format)\n",
    "# fig.savefig(fig_file_name, format=fig_format, dpi=fig_dpi)\n",
    "\n",
    "_ = plot_comparison_bars(zs_total_expert_models_agreement[gs_analysis.model_names], \n",
    "                         gs_total_expert_models_agreement, \n",
    "                         gs_analysis, \n",
    "                        #  filename=fig_file_name, dpi=fig_dpi,\n",
    "                         figsize=(12,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig_title = 'Expert-Model inter-rater agreement on individual question scores for ZS vs GS prompting'\n",
    "# fig_file_name = os.path.join(figures_dir, fig_title + '.' + fig_format)\n",
    "# fig.savefig(fig_file_name, format=fig_format, dpi=fig_dpi)\n",
    "\n",
    "plot_multiple_comparisons(zs_qw_expert_models_agreement, \n",
    "                          gs_qw_expert_models_agreement, \n",
    "                          gs_analysis, \n",
    "                        #   filename=fig_file_name, dpi=fig_dpi,\n",
    "                          ncols=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig_title = \"Average of models' question-wise ZS and GS prompting agreement\"\n",
    "# fig_file_name = os.path.join(figures_dir, fig_title + '.' + fig_format)\n",
    "# fig.savefig(fig_file_name, format=fig_format, dpi=fig_dpi)\n",
    "\n",
    "zs_mean_df = zs_qw_expert_models_agreement.mean(axis=1).to_frame(name='mean').T\n",
    "gs_mean_df = gs_qw_expert_models_agreement.mean(axis=1).to_frame(name='mean').T\n",
    "\n",
    "_ = plot_comparison_bars(zs_mean_df, \n",
    "                         gs_mean_df, \n",
    "                         gs_analysis, \n",
    "                        #  filename=fig_file_name, dpi=fig_dpi,\n",
    "                         figsize=(20,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = questionwise_agreement_heatmap(gs_qw_expert_models_agreement, figsize=(12,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying samples of responses by LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'GPT-4o'\n",
    "\n",
    "def get_model_responses(model_name, models_dir, analysis_instance, model_files_dict) -> pd.DataFrame:\n",
    "    model_file_path = os.path.join(models_dir, model_files_dict[model_name])\n",
    "    model_responses_df = pd.read_csv(model_file_path, encoding='utf-8')\n",
    "    return merge_dataframes(model_responses_df, analysis_instance.group_df, ['Video ID'] + EXPERTS_AVG_COLUMNS)\n",
    "\n",
    "zs_model_responses_df = get_model_responses(model_name, models_dir, zs_analysis, zs_model_files_dict)\n",
    "gs_model_responses_df = get_model_responses(model_name, models_dir, gs_analysis, gs_model_files_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_display_examples(zs_model_responses_df: pd.DataFrame, q_num, expert_score, example_num, diff, model_name):\n",
    "    # Define the range for expert scores and differences\n",
    "    expert_score_range = [expert_score, expert_score - 0.5]\n",
    "    diff_range = [diff, diff - 0.5]\n",
    "\n",
    "    # Filter rows where the expert score is within the defined range\n",
    "    filtered_to_expert_score_df = zs_model_responses_df[\n",
    "        zs_model_responses_df[f'Experts_Avg Q{q_num}'].isin(expert_score_range)\n",
    "    ]\n",
    "\n",
    "    # Check if the dataframe is not empty\n",
    "    if not filtered_to_expert_score_df.empty:\n",
    "        # Filter rows where the absolute difference between model score and expert score is within the defined range\n",
    "        filtered_to_diff_df = filtered_to_expert_score_df[\n",
    "            filtered_to_expert_score_df.apply(\n",
    "                lambda row: any(abs(row[f'Q{q_num}'] - row[f'Experts_Avg Q{q_num}']) == d for d in diff_range),\n",
    "                axis=1\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        if filtered_to_diff_df.empty:\n",
    "            print(f'No example found where the expert score is within {expert_score_range} and the difference is within {diff_range}')\n",
    "        else:\n",
    "            number_of_examples = len(filtered_to_diff_df)\n",
    "            print(f'There are {number_of_examples} example(s) for the selected difference and expert score')\n",
    "            if example_num > number_of_examples:\n",
    "                print(f\"Error: `example_num` should be {number_of_examples} or less\")\n",
    "            else:\n",
    "                example_row = filtered_to_diff_df.iloc[example_num - 1]\n",
    "                transcript = example_row['Transcript']\n",
    "                response = example_row[f'Response_{q_num}']\n",
    "                q_score = example_row[f'Q{q_num}']\n",
    "                avg_score = example_row[f'Experts_Avg Q{q_num}']\n",
    "                actual_diff = abs(q_score - avg_score)\n",
    "                print('Video ID:', example_row['Video ID'])\n",
    "                display_wrapped(\"Transcript: \" + transcript)\n",
    "                print(f'{model_name} Response_{q_num} where Experts_Avg Q{q_num}={avg_score} and Q{q_num}={q_score} have a difference of {actual_diff:.1f}:')\n",
    "                display_wrapped(response)\n",
    "    else:\n",
    "        print(f'No examples found where the expert score is within {expert_score_range}')\n",
    "\n",
    "question_num = 9\n",
    "expert_score = 4\n",
    "difference = 4            # 0 for high agreement, 2 for medium, and 4 for low\n",
    "example_num = 1     # from 1 to len(filtered_to_expert_score_df)\n",
    "filter_and_display_examples(zs_model_responses_df, question_num, expert_score, example_num, difference, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video IDs where the agreement on ZS is better than the agreement on GS prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_ids_for_condition(zs_model_responses_df, gs_model_responses_df, q_num, diff_limit=1.5):\n",
    "    \"\"\"\n",
    "    Find Video IDs where the absolute difference for a specific question number is less than `diff_limit`\n",
    "    in zs_model_responses_df but greater than `diff_limit` in gs_model_responses_df.\n",
    "    \"\"\"\n",
    "    # Calculate the absolute differences\n",
    "    zs_diff = abs(zs_model_responses_df[f'Q{q_num}'] - zs_model_responses_df[f'Experts_Avg Q{q_num}'])\n",
    "    gs_diff = abs(gs_model_responses_df[f'Q{q_num}'] - gs_model_responses_df[f'Experts_Avg Q{q_num}'])\n",
    "    \n",
    "    # Find the indices that meet the conditions\n",
    "    indices = zs_model_responses_df.index[(zs_diff < diff_limit) & (gs_diff > diff_limit)]\n",
    "    \n",
    "    # Get the corresponding Video IDs\n",
    "    video_ids = zs_model_responses_df.loc[indices, 'Video ID']\n",
    "    \n",
    "    return video_ids.tolist()\n",
    "\n",
    "# Example usage\n",
    "question_num = 5\n",
    "difference_limit = 1.5\n",
    "video_ids = get_video_ids_for_condition(zs_model_responses_df, gs_model_responses_df, question_num, difference_limit)\n",
    "print(f'Video IDs meeting the condition:', video_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display Expert, ZS, and GS Responses for Sample Video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaple_video_id = 'OO5oDaG45kE'\n",
    "cell_index = zs_model_responses_df.loc[zs_model_responses_df['Video ID'] == smaple_video_id].index[0]\n",
    "print('Expert score:', zs_model_responses_df.at[cell_index, f'Experts_Avg Q{question_num}'], end='\\n')\n",
    "\n",
    "zs_response = zs_model_responses_df.at[cell_index, f'Response_{question_num}']\n",
    "display_wrapped('ZS response: ' + zs_response)\n",
    "\n",
    "print(\"########################################################################\")\n",
    "\n",
    "gs_response = gs_model_responses_df.at[cell_index, f'Response_{question_num}']\n",
    "display_wrapped('GS response: ' + gs_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
