{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove prompt from early LLMs responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_head = \"You are a medical expert. Rate the following transcript of a YouTube video according to this question:\"\n",
    "questions = [\n",
    "    \"Are the aims of the video clear?\",\n",
    "    \"Does the video achieve it's aims?\",\n",
    "    \"Is the video relevant?\",\n",
    "    \"Is the video clear what sources of information were used to compile the publication (other than the author)?\",\n",
    "    \"Is the video clear when the information used or reported in the transcript was produced?\",\n",
    "    \"Is the video balanced and unbiased?\",\n",
    "    \"Does the video provide details of additional sources of support and information?\",\n",
    "    \"Does the video refer to areas of uncertainty?\",\n",
    "    \"Does the video describe how each treatment works?\",\n",
    "    \"Does the video describe the benefits of each treatment?\",\n",
    "    \"Does the video describe the risks of each treatment?\",\n",
    "    \"Does the video describe what would happen if no treatment is used?\",\n",
    "    \"Does the video describe how the treatment choices affect overall quality of life?\",\n",
    "    \"Is the video clear that there may be more than one possible treatment choice?\",\n",
    "    \"Does the video provide support for shared decision-making?\",\n",
    "]\n",
    "question_tail = \"Return an integer score from 1 to 5, where 1 means 'no', 2 to 4 means 'partially', and 5 means 'yes'. Then, explain your choice.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_prompt_from_output(output):\n",
    "    \"\"\"Check if the prompt exists at the beginning of llm_output and remove it\"\"\"\n",
    "    # if output.startswith(prompt[:20]):\n",
    "        # prompt_end = prompt[-7:]        # Get the last 7 characters of the prompt\n",
    "    prompt_end = \"Score:\"\n",
    "    idx = output.find(prompt_end)   # Find the index of the last characters of the prompt in the output\n",
    "    if idx != -1:\n",
    "        output = output[idx + len(prompt_end):]  # take response after the prompt\n",
    "    return output\n",
    "\n",
    "def check_repetition(text, min_words=5, min_occurrences=2):\n",
    "    \"\"\"\n",
    "    Find repeated sentences longer than 'min_words' words, occurring more than 'min_occurrences' times in the given text.\n",
    "    Some models are repeating some phrases in their output.    \n",
    "    \"\"\"\n",
    "    # Split text into sentences\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    \n",
    "    phrase_counter = {}             # Initialize a dictionary to store occurrences of phrases\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()        # Tokenize the sentence into words\n",
    "        \n",
    "        # Generate all possible phrases longer than min_length words\n",
    "        for i in range(len(words) - min_words + 1):\n",
    "            phrase = ' '.join(words[i:i+min_words])\n",
    "            # Update the counter for this phrase\n",
    "            if phrase in phrase_counter:\n",
    "                phrase_counter[phrase] += 1\n",
    "            else:\n",
    "                phrase_counter[phrase] = 1\n",
    "    \n",
    "    # Filter phrases that occur more than min_occurrences times\n",
    "    repeated_phrases = [phrase for phrase, count in phrase_counter.items() if count > min_occurrences]\n",
    "    \n",
    "    if len(repeated_phrases):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def extract_and_store_score(llm_output, df, video_id, question_num):\n",
    "    \"\"\"\n",
    "    Extract score integer from beginning of LLM response.\n",
    "    NOTE: Prompt should have been removed already from the response beginning.\n",
    "    \"\"\"\n",
    "    questions_with_problem = []\n",
    "    pattern = r'([0-5])'\n",
    "    match = re.search(pattern, llm_output)  # Search for the first encountered integer\n",
    "    if match:\n",
    "        score = int(match.group(1))\n",
    "        if 1 <= score <= 5:\n",
    "            df.loc[df['Video ID'] == video_id, f'Q{question_num}'] = score\n",
    "        elif score == 0:\n",
    "            df.loc[df['Video ID'] == video_id, f'Q{question_num}'] = 1\n",
    "    else:\n",
    "        questions_with_problem.append(question_num)\n",
    "        df.loc[df['Video ID'] == video_id, 'Problem'] = questions_with_problem\n",
    "        # df.loc[df['Video ID'] == video_id, 'Problem'] = True\n",
    "    \n",
    "    if question_num == 15:\n",
    "        columns_to_check = [f'Q{i}' for i in range(1, 16)]\n",
    "        # Filter the DataFrame for the specific video_id\n",
    "        filtered_df = df[df['Video ID'] == video_id]\n",
    "        # Count the occurrences of 1 in columns Q1 to Q15 for the filtered DataFrame\n",
    "        count_ones = sum(filtered_df[column].eq(1).sum() for column in columns_to_check)\n",
    "        if count_ones >= 8:   # if there are many ones then probably prompt was not removed from response\n",
    "            df.loc[df['Video ID'] == video_id, 'Problem'] = True\n",
    "\n",
    "def store_and_check_response(llm_output, question_num, llm_scores_df, video_id, print_response=True):\n",
    "    if print_response:\n",
    "        print(f\"Q{question_num} response: {llm_output}\")# if question_num == 5 else None\n",
    "        \n",
    "    llm_scores_df.loc[llm_scores_df['Video ID'] == video_id, f'Response_{question_num}'] = llm_output     # store response\n",
    "    llm_scores_df.loc[llm_scores_df['Video ID'] == video_id, 'Problem'] = check_repetition(llm_output)    # check for repeated phrases\n",
    "    extract_and_store_score(llm_output, llm_scores_df, video_id, question_num)                            # extract and store score\n",
    "\n",
    "def check_prompt_in_response(response):\n",
    "    if 'Instruction' in response \\\n",
    "        and question_head in response \\\n",
    "        and question_tail in response \\\n",
    "        and 'Transcript: ' in response \\\n",
    "        and 'Score:' in response:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "dir = './Original_Responses'\n",
    "files = os.listdir('./Original_Responses')\n",
    "csv_files = [file for file in files if file.endswith('.csv')]\n",
    "\n",
    "for file in csv_files:\n",
    "    path_to_file = dir + '/' + file\n",
    "    response_df = pd.read_csv(path_to_file, encoding='utf-8')\n",
    "    for video_id in response_df['Video ID'].values:\n",
    "        \n",
    "        for question_num in range(1,16):\n",
    "            response = str(response_df.loc[response_df['Video ID'] == video_id, f'Response_{question_num}'].iloc[0])\n",
    "        \n",
    "            if check_prompt_in_response(response):\n",
    "                print(f\"File: {file} | video ID: {video_id} | Question {question_num}\")\n",
    "                response = remove_prompt_from_output(response)\n",
    "                store_and_check_response(response, question_num, response_df, video_id, print_response=False)\n",
    "\n",
    "    new_file_path = './Fixed_Responses/' + file\n",
    "    response_df.to_csv(new_file_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert all 'True'/'False' in `Problem` column to a list along with question_nums that have a problem\n",
    "#### Add `Transcripts` column to `responses_df`\n",
    "Problem is either there is no integer or there is repetition in the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../../../Codes/LLM_Evaluations')  # Add the parent directory of LLM_Evaluations to the Python path\n",
    "\n",
    "from llm_evaluation_utils import \\\n",
    "                        extract_score,   \\\n",
    "                        check_repetition,      \\\n",
    "                        QUESTION_HEAD, QUESTIONS, QUESTION_TAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts_file_path = \"../../../Codes/Getting_Transcripts/filtered_videos_transcripts.csv\"\n",
    "responses_dir = \"..\"\n",
    "\n",
    "videos_df = pd.read_csv(transcripts_file_path, usecols=['Video ID', 'Transcript'], encoding='utf-8')\n",
    "files_in_directory = os.listdir(responses_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in files_in_directory:\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(responses_dir, file_name)\n",
    "        responses_df = pd.read_csv(file_path, encoding='utf-8')\n",
    "        if 'Transcript' not in responses_df.columns:\n",
    "            responses_df.insert(1, 'Transcript', videos_df['Transcript']) \n",
    "        print(file_name)\n",
    "\n",
    "        problem_sets = pd.Series(dtype='object')\n",
    "\n",
    "        for index, row in responses_df.iterrows():\n",
    "            questions_with_problem = set()\n",
    "            for i in range(1,16):\n",
    "                response = row[f'Response_{i}']\n",
    "                # print(index, i)\n",
    "                if pd.isna(response) or extract_score(response) is None or check_repetition(response):\n",
    "                    questions_with_problem.add(i)\n",
    "                \n",
    "                cell = row[f'Q{i}']\n",
    "                if isinstance(cell, float) and pd.notna(cell):\n",
    "                    # Convert the float to an integer\n",
    "                    responses_df.at[index, f'Q{i}'] = int(cell)\n",
    "\n",
    "            problem_sets.at[index] = questions_with_problem\n",
    "\n",
    "        # Assign the Series to the DataFrame as a new column 'Problem'\n",
    "        responses_df['Problem'] = problem_sets\n",
    "        \n",
    "        responses_df.to_csv(file_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in files_in_directory:\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(responses_dir, file_name)\n",
    "        responses_df = pd.read_csv(file_path, encoding='utf-8')\n",
    "\n",
    "        for i in range(1,16):\n",
    "            responses_df[f'Q{i}'] = pd.to_numeric(responses_df[f'Q{i}'], errors='coerce', downcast='integer').astype('Int64')\n",
    "\n",
    "        print(responses_df.head())\n",
    "        responses_df.to_csv(file_path, index=False, encoding='utf-8')\n",
    "        time.sleep(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
