{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Required Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages for `LambdaLabs` Cloud GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required for Lambda Labs\n",
    "\n",
    "!pip -q install --upgrade pip\n",
    "!pip -q install python-dotenv\n",
    "!pip -q install --upgrade numexpr bottleneck scipy            # required for pandas in LambdaLabs\n",
    "!pip -q install --upgrade pandas transformers bitsandbytes accelerate\n",
    "# !pip -q install xformers einops optimum sentencepiece         # required for some models\n",
    "!pip install gpustat                                          # to watch GPU utilization and memory\n",
    "!pip install langdetect                                       # to check response language\n",
    "\n",
    "# !pip show transformers torch                                    # transformers should be > 4.40, torch >= 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install flash attention 2. Make computations faster for some models\n",
    "!pip uninstall -y ninja && pip install ninja\n",
    "!pip install --upgrade torch==2.1\n",
    "!pip install https://github.com/Dao-AILab/flash-attention/releases/download/v2.5.7/flash_attn-2.5.7+cu122torch2.1cxx11abiFALSE-cp310-cp310-linux_x86_64.whl\n",
    "\n",
    "# or:\n",
    "# !yes | git clone git@github.com:Dao-AILab/flash-attention.git\n",
    "# !python ./flash-attention/setup.py install\n",
    "# !pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To install a model locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First\n",
    "!sudo apt-get install git-lfs\n",
    "!git lfs install\n",
    "\n",
    "# Then\n",
    "!git clone https://huggingface.co/mistralai/Mistral-7B-v0.2\n",
    "\n",
    "# Finally\n",
    "model_id = './Mistral-7B-v0.2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To clean CPU memory and remove models that were downloaded:\n",
    "In CLI:  \n",
    "```\n",
    "ls ~/.cache/huggingface/hub\n",
    "rm -rf ~/.cache/huggingface/hub/[model_folder]\n",
    "```\n",
    "If GPU memory is not enough while inferencing, try the following in Jupyter Notebook then restart kernel  \n",
    "```\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, BitsAndBytesConfig#, pipeline\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')  # Add the parent directory of LLM_Evaluations to the Python path\n",
    "\n",
    "from Utils.llm_evaluation_utils import load_responses_df,         \\\n",
    "                        check_and_store_response,   \\\n",
    "                        QUESTION_SETS\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "\n",
    "load_dotenv()\n",
    "access_token = os.environ.get('HF_API_KEY')\n",
    "\n",
    "login(access_token)\n",
    "question_type = 'GS'\n",
    "QUESTION_HEAD = QUESTION_SETS[question_type]['QUESTION_HEAD']\n",
    "QUESTIONS = QUESTION_SETS[question_type]['QUESTIONS']\n",
    "QUESTION_TAIL = QUESTION_SETS[question_type]['QUESTION_TAIL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initially I used `pipeline` from `transformers` for inference. \n",
    "However, configuring the pipeline directly is deprecated. Generation Config should be used in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def initialize_model_and_tokenizer(model_id, task, max_length=4096, torch_dtype='auto', load_in_8bit=False):\n",
    "    '''Initialize model and tokenizer.'''\n",
    "    # bits_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "    # gptq_config = GPTQConfig(bits=4)\n",
    "    model_kwargs={'max_length': max_length, 'load_in_8bit': load_in_8bit}\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    return pipeline(model=model_id,\n",
    "                    task=task,\n",
    "                    tokenizer=tokenizer,\n",
    "                    torch_dtype=torch_dtype, #torch.bfloat16,\n",
    "                    device=0,\n",
    "                    # device_map='auto',\n",
    "                    model_kwargs=model_kwargs,\n",
    "                    trust_remote_code=True,\n",
    "                    # quantization_config=bnb_config,\n",
    "                    )\n",
    "\n",
    "def run_llm(pipe):\n",
    "    '''Generate response from the language model pipeline.'''\n",
    "    return pipe(\n",
    "                prompt,\n",
    "                # max_new_tokens=30,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=pipe.tokenizer.eos_token_id,\n",
    "                )[0]['generated_text']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Models\n",
    "Each model has its configuration parameters and prompt style to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    {\n",
    "        'model_name': 'falcon-7b-instruct',\n",
    "        'model_id': 'tiiuae/falcon-7b-instruct',\n",
    "        'task': 'text-generation',\n",
    "        'max_length': 2048,\n",
    "        'prompt_template': f'Instruction: {{instruction}}\\nTranscript: \"{{transcript}}\"\\nScore:',\n",
    "        'quantize': None,\n",
    "        'torch_dtype': 'auto',\n",
    "    },\n",
    "    {\n",
    "        'model_name': 'falcon-40b-instruct',\n",
    "        'model_id': 'tiiuae/falcon-40b-instruct',\n",
    "        'task': 'text-generation',\n",
    "        'max_length': 2048,\n",
    "        'prompt_template': f'Instruction: {{instruction}}\\nTranscript: \"{{transcript}}\"\\nScore:',\n",
    "        'quantize': 'int4',\n",
    "        'torch_dtype': 'auto',\n",
    "    },\n",
    "    {\n",
    "        'model_name': 'Mistral-7B-Instruct',\n",
    "        'model_id': 'mistralai/Mistral-7B-Instruct-v0.2',\n",
    "        'task': 'text-generation',\n",
    "        'max_length': 1000000000000000019884624838656,\n",
    "        'prompt_template': f'[INST]Instruction: {{instruction}}\\nTranscript:\"{{transcript}}\"\\nScore:[/INST]',\n",
    "        'quantize': None,\n",
    "        'torch_dtype': 'auto',\n",
    "    },\n",
    "    {\n",
    "        'model_name': 'Mixtral-8x7B-Instruct',\n",
    "        'model_id': 'mistralai/Mixtral-8x7B-Instruct-v0.1',\n",
    "        'task': 'text-generation',\n",
    "        'max_length': 1000000000000000019884624838656,\n",
    "        'prompt_template': f'[INST]Instruction: {{instruction}}\\nTranscript:\"{{transcript}}\"\\nScore:[/INST]',\n",
    "        'quantize': 'int4',\n",
    "        'torch_dtype': 'auto',\n",
    "        # attn_implementation='flash_attention_2',\n",
    "    },\n",
    "    {\n",
    "        'model_name': 'phi-2',\n",
    "        'model_id': 'microsoft/phi-2',\n",
    "        'task': 'text-generation',\n",
    "        'max_length': 2048,\n",
    "        'prompt_template': f'Instruction: {{instruction}}\\nTranscript: \"{{transcript}}\"\\nScore:',\n",
    "        'quantize': None,\n",
    "        'torch_dtype': 'auto',\n",
    "    },\n",
    "    {\n",
    "        'model_name': 'AdaptLLM-medicine',\n",
    "        'model_id': 'AdaptLLM/medicine-LLM',\n",
    "        'task': 'text-generation',\n",
    "        'max_length': 2048,\n",
    "        'prompt_template': f'Instruction: {{instruction}}\\nTranscript: \"{{transcript}}\"\\nScore:',\n",
    "        'quantize': None,\n",
    "        'torch_dtype': torch.float16,\n",
    "    },\n",
    "    {\n",
    "        'model_name': 'Yi-34B',\n",
    "        'model_id': '01-ai/Yi-34B',\n",
    "        'task': 'text-generation',\n",
    "        'max_length': 4096,\n",
    "        'prompt_template': f'# Instruction: {{instruction}}\\nTranscript: \"{{transcript}}\"\\nScore:',\n",
    "        'quantize': None,\n",
    "        'torch_dtype': torch.float16,\n",
    "    },\n",
    "    {\n",
    "        'model_name': 'Qwen1_5-72B',\n",
    "        'model_id': 'Qwen/Qwen1.5-72B',\n",
    "        'task': 'text-generation',\n",
    "        'max_length': 32768,\n",
    "        'prompt_template': f'Instruction: {{instruction}}\\nTranscript: \"{{transcript}}\"\\nScore:',\n",
    "        'quantize': 'int4',\n",
    "        'torch_dtype': 'auto',\n",
    "    },\n",
    "    {\n",
    "        'model_name': 'mpt-30b',\n",
    "        'model_id': 'mosaicml/mpt-30b',\n",
    "        'task': 'text-generation',\n",
    "        'max_length': 8192,\n",
    "        'prompt_template': f'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n###Instruction: {{instruction}}\\nTranscript: \"{{transcript}}\"\\n\\n### Response: Score:',\n",
    "        'quantize': None,\n",
    "        'torch_dtype': 'auto',\n",
    "    },\n",
    "    {\n",
    "        'model_name': 'Xwin-70B',   # did not complete inference on the whole dataset\n",
    "        'model_id': 'Xwin-LM/Xwin-LM-70B-V0.1',\n",
    "        'task': 'text-generation',\n",
    "        'max_length': 4096,\n",
    "        'prompt_template': f'''A chat between a curious user and an artificial intelligence assistant. \n",
    "The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
    "USER: Instruction: {{instruction}}\n",
    "Transcript: \"{{transcript}}\"\n",
    "ASSISTANT: Score:''',\n",
    "        'quantize': 'int4',\n",
    "        'torch_dtype': 'auto',\n",
    "        #    do_sample=True,\n",
    "    },\n",
    "    {\n",
    "        'model_name': 'vicuna-33b',\n",
    "        'model_id': 'lmsys/vicuna-33b-v1.3',\n",
    "        'task': 'text-generation',\n",
    "        'max_length': 2048,\n",
    "        'prompt_template': f'''A chat between a curious user and an artificial intelligence assistant. \n",
    "The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
    "USER: Instruction: {{instruction}}\n",
    "Transcript: \"{{transcript}}\"\n",
    "ASSISTANT: Score:''',\n",
    "        'quantize': None,\n",
    "        'torch_dtype': 'auto',\n",
    "    },\n",
    "    {\n",
    "        'model_name': 'Orca-2',\n",
    "        'model_id': 'microsoft/Orca-2-13b',\n",
    "        'task': 'text-generation',\n",
    "        'max_length': 4096,\n",
    "        # 'system': 'You are Orca, an AI language model created by Microsoft. You are a cautious assistant. You carefully follow instructions.',\n",
    "        # 'user': f'Instruction: {{instruction}}\\nTranscript: \"{{transcript}}\"\\nScore:',\n",
    "        # 'prompt_template': f'<|im_start|>system\\n{{system}}<|im_end|>\\n<|im_start|>user\\n{{user}}<|im_end|>\\n<|im_start|>assistant',\n",
    "        # prompt_template = matching_model['prompt_template'].format(system=matching_model['system'], user=matching_model['user'])\n",
    "        'prompt_template': f'Instruction: {{instruction}}\\nTranscript: \"{{transcript}}\"\\nScore:',\n",
    "        'quantize': None,\n",
    "        'torch_dtype': 'auto'\n",
    "    },\n",
    "    {\n",
    "        'model_name': 'KTO_Mistral_PairRM', #7B\n",
    "        'model_id': 'ContextualAI/Contextual_KTO_Mistral_PairRM',\n",
    "        'task': 'text-generation',\n",
    "        'max_length': 2048,\n",
    "        'prompt_template': f'<|user|>\\nInstruction: {{instruction}}\\nTranscript: \"{{transcript}}\"\\n\\n<|assistant|>\\nScore:',\n",
    "        'quantize': None,\n",
    "        'torch_dtype': 'auto',\n",
    "    },\n",
    "    {\n",
    "        'model_name': 'Ein-72B',\n",
    "        'model_id': 'SF-Foundation/Ein-72B-v0.1-full',\n",
    "        'task': 'text-generation',\n",
    "        'max_length': 4096,\n",
    "        'prompt_template': f'Instruction: {{instruction}}\\nTranscript: \"{{transcript}}\"\\nScore:',\n",
    "        'quantize': 'int4',\n",
    "        'torch_dtype': 'auto',\n",
    "    },\n",
    "    {\n",
    "        'model_name': 'Mixtral-8x22B-Instruct',\n",
    "        'model_id': 'mistralai/Mixtral-8x22B-Instruct-v0.1',\n",
    "        'task': 'text-generation',\n",
    "        'max_length': 1000000000000000019884624838656,\n",
    "        'prompt_template': f'Instruction: {{instruction}}\\nTranscript: \"{{transcript}}\"\\nScore:',\n",
    "        # 'prompt_template': f'[INST]Instruction: {{instruction}}\\nTranscript:\"{{transcript}}\\nScore:[/INST]',\n",
    "        'quantize': 'int4',\n",
    "        'torch_dtype': 'auto',\n",
    "    },\n",
    "    {\n",
    "        'model_name': 'Llama-3-8B-Instruct',\n",
    "        'model_id': 'meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "        'task': 'text-generation',\n",
    "        'max_length': 1000000000000000019884624838656,\n",
    "        # 'prompt_template': f'Instruction: {{instruction}}\\nTranscript: \"{{transcript}}\"\\nScore:',\n",
    "        'user': f'Instruction: {{instruction}}\\nTranscript: \"{{transcript}}\"\\nScore:',\n",
    "        'prompt_template': f'''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\n",
    "{{user}}<|eot_id|><|start_header_id|>assistant<|end_header_id|>''',\n",
    "#         'system': f'{{instruction}}',\n",
    "#         'user': f'Transcript: \"{{transcript}}\"\\nScore:',\n",
    "#         'prompt_template': f'''<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\n",
    "# {{system}}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\n",
    "# {{user}}<|eot_id|><|start_header_id|>assistant<|end_header_id|>''',\n",
    "        'quantize': None,\n",
    "        'torch_dtype': torch.float16,\n",
    "    },\n",
    "    {\n",
    "        'model_name': 'Llama-3-70B-Instruct',\n",
    "        'model_id': 'meta-llama/Meta-Llama-3-70B-Instruct',\n",
    "        'task': 'text-generation',\n",
    "        'max_length': 1000000000000000019884624838656,\n",
    "        # 'prompt_template': f'Instruction: {{instruction}}\\nTranscript: \"{{transcript}}\"\\nScore:',\n",
    "        'user': f'{{instruction}}\\nTranscript: \"{{transcript}}\"\\nScore:',\n",
    "        'prompt_template': f'''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\n",
    "{{user}}<|eot_id|><|start_header_id|>assistant<|end_header_id|>''',\n",
    "#         'system': f'{{instruction}}',\n",
    "#         'user': f'Transcript: \"{{transcript}}\"\\nScore:',\n",
    "#         'prompt_template': f'''<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\n",
    "# {{system}}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\n",
    "# {{user}}<|eot_id|><|start_header_id|>assistant<|end_header_id|>''',\n",
    "        'quantize': 'int4',\n",
    "        'torch_dtype': 'auto',\n",
    "    },\n",
    "    {\n",
    "        'model_name': 'Rhea',\n",
    "        'model_id': 'davidkim205/Rhea-72b-v0.5',\n",
    "        'task': 'text-generation',\n",
    "        'max_length': 4096,\n",
    "        'prompt_template': f'Instruction: {{instruction}}\\nTranscript: \"{{transcript}}\"\\nScore:',\n",
    "        'quantize': 'int4',\n",
    "        'torch_dtype': 'auto',\n",
    "    },\n",
    "    {\n",
    "        'model_name': 'MultiVerse_70B',\n",
    "        'model_id': 'MTSAIR/MultiVerse_70B',\n",
    "        'task': 'text-generation',\n",
    "        'max_length': 4096,\n",
    "        'prompt_template': f'### Instruction: {{instruction}}\\nTranscript: \"{{transcript}}\"\\n### Response: Score:',\n",
    "        'quantize': 'int4',\n",
    "        'torch_dtype': 'auto',\n",
    "    },\n",
    "    # {\n",
    "    #     'model_name': 'Smaug-72B',        # I did not try this one\n",
    "    #     'model_id': 'abacusai/Smaug-72B-v0.1',\n",
    "    #     'task': 'text-generation',\n",
    "    #     'max_length': 4096,\n",
    "    #     'prompt_template': f'[INST]<<SYS>>Instruction: {{instruction}}<</SYS>>\\nTranscript: \"{{transcript}}\"[/INST]\\nScore:',\n",
    "    #     'quantize': 'int4',\n",
    "    #     'torch_dtype': 'auto',\n",
    "    # },\n",
    "    {\n",
    "        'model_name': 'BioMistral',\n",
    "        'model_id': 'BioMistral/BioMistral-7B',\n",
    "        'task': 'text-generation',\n",
    "        'max_length': 1000000000000000019884624838656,\n",
    "        'prompt_template': f'Instruction: {{instruction}}\\nTranscript: \"{{transcript}}\"\\nScore:',\n",
    "        'quantize': None,\n",
    "        'torch_dtype': 'auto',\n",
    "    },\n",
    "    {\n",
    "        'model_name': 'meerkat',\n",
    "        'model_id': 'dmis-lab/meerkat-7b-v1.0',\n",
    "        'task': 'text-generation',\n",
    "        'max_length': 1000000000000000019884624838656,\n",
    "        'prompt_template': f'### Instruction: {{instruction}}\\nTranscript: \"{{transcript}}\"\\nScore:',\n",
    "        'quantize': None,\n",
    "        'torch_dtype': 'auto',\n",
    "    },\n",
    "    {\n",
    "        'model_name': 'Phi-3-mini-128k-instruct',\n",
    "        'model_id': 'microsoft/Phi-3-mini-4k-instruct',\n",
    "        'task': 'text-generation',\n",
    "        'max_length': 4096,\n",
    "        'user': f'Instruction: {{instruction}}\\nTranscript: \"{{transcript}}\"\\nScore:',\n",
    "        'prompt_template': f'<|user|>\\n{{user}}<|end|>\\n<|assistant|>',\n",
    "        # 'prompt_template': f'Instruction: {{instruction}}\\nTranscript: \"{{transcript}}\"\\nScore:',\n",
    "        'quantize': None,\n",
    "        'torch_dtype': 'auto',\n",
    "    },\n",
    "    {\n",
    "        'model_name': 'meditron',           # hallucinations\n",
    "        'model_id': 'epfl-llm/meditron-70b',\n",
    "        'task': 'text-generation',\n",
    "        'max_length': 1000000000000000019884624838656,\n",
    "        # 'prompt_template': f'Instruction: {{instruction}}\\nTranscript: \"{{transcript}}\"\\nScore:',\n",
    "        'system': f'{{instruction}}',\n",
    "        'user':f'Transcript: \"{{transcript}}\"\\nScore:',\n",
    "        'quantize': 'int4',\n",
    "        'torch_dtype': torch.float16,\n",
    "    },    \n",
    "    {\n",
    "        'model_name': 'JSL-Med',        # rubish\n",
    "        'model_id': 'johnsnowlabs/JSL-Med-Sft-Llama-3-8B',\n",
    "        'task': 'text-generation',\n",
    "        'max_length': 1000000000000000019884624838656,\n",
    "        'prompt_template': f'Instruction: {{instruction}}\\nTranscript: \"{{transcript}}\"\\nScore:',\n",
    "        'quantize': None,\n",
    "        'torch_dtype': 'auto',\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting and Loading a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Orca-2'\n",
    "\n",
    "try:\n",
    "    matching_model = next((model for model in models if model['model_name'] == model_name), None)\n",
    "\n",
    "    if matching_model is not None:\n",
    "        model_id = matching_model['model_id']\n",
    "        print('Loading model:', model_id)\n",
    "        \n",
    "        # Preparing prompt template\n",
    "        prompt_template = matching_model['prompt_template']\n",
    "        if 'system' in matching_model and 'user' in matching_model:\n",
    "            prompt_template = prompt_template.format(\n",
    "                system=matching_model['system'], user=matching_model['user'])\n",
    "        elif 'user' in matching_model:\n",
    "            prompt_template = prompt_template.format(user=matching_model['user'])\n",
    "\n",
    "        quantize = matching_model.get('quantize', None)\n",
    "        torch_dtype = matching_model.get('torch_dtype', 'auto')\n",
    "\n",
    "        # Configuring quantization\n",
    "        if quantize in ['int8', 'int4']:\n",
    "            if quantize == 'int8':\n",
    "                bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "            elif quantize == 'int4':\n",
    "                bnb_config = BitsAndBytesConfig(\n",
    "                    load_in_4bit=True,\n",
    "                    bnb_4bit_use_double_quant=True,\n",
    "                    bnb_4bit_quant_type='nf4',\n",
    "                    bnb_4bit_compute_dtype=torch.float16\n",
    "                )\n",
    "        else:\n",
    "            bnb_config = None\n",
    "\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        torch.set_default_device(device)\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=device,\n",
    "            torch_dtype=torch_dtype,\n",
    "            trust_remote_code=True,\n",
    "            quantization_config=bnb_config\n",
    "        )\n",
    "    else:\n",
    "        print('Model information not found for:', model_name)\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_first_empty_string_row_index(responses_df):\n",
    "    responses_columns = [f'Response_{i}' for i in range(1, len(QUESTIONS))]\n",
    "\n",
    "    # Check for NaN or empty string values in the response columns\n",
    "    nan_or_empty_indices = responses_df[responses_columns].isna() | (responses_df[responses_columns] == '')\n",
    "\n",
    "    # Check if any NaN or empty string values exist\n",
    "    if nan_or_empty_indices.any().any():\n",
    "        # Find the index of the first row with NaN or empty string values\n",
    "        return nan_or_empty_indices.any(axis=1).idxmax()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "transcripts_dir = '../../Getting_Transcripts'\n",
    "transcripts_file_name = 'merged_filtered_videos_transcripts.csv'\n",
    "responses_dir = '../../../Results/LLMs_Responses'\n",
    "topics_to_include = ['Spina Bifida', 'Flat Feet', 'Cluster Headache', 'Trigger Finger', 'Pudendal Nerve']\n",
    "\n",
    "prompt_type = 'GS_prompting'\n",
    "topics = 'last_5_topics'\n",
    "results_file_name = f'{model_name}-{topics}-{prompt_type}'\n",
    "\n",
    "responses_df = load_responses_df(transcripts_dir, transcripts_file_name, responses_dir, results_file_name, question_type)\n",
    "\n",
    "print(responses_df.shape)\n",
    "responses_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Topic' not in responses_df.columns:\n",
    "    experts_file = '../../../Videos_and_DISCERN_data/filtered_experts_scores.csv'\n",
    "    experts_df = pd.read_csv(experts_file)\n",
    "\n",
    "    responses_df = responses_df.merge(experts_df[['Video ID', 'Topic']], on='Video ID', how='left')\n",
    "    responses_df.insert(2, 'Topic', responses_df.pop('Topic'))\n",
    "    responses_df = responses_df[responses_df['Topic'].isin(topics_to_include)]\n",
    "    responses_df = responses_df.reset_index(drop=True)\n",
    "\n",
    "print('responses_df shape:', responses_df.shape)\n",
    "responses_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=250,\n",
    "    # num_beams=2,                  # default is 1\n",
    "    # early_stopping=True,\n",
    "    do_sample=False,\n",
    "    # temperature = 0.8,            # set between 0.5-1 if do_sample=True\n",
    "    # repetition_penalty=1.2,\n",
    "    eos_token_id=model.config.eos_token_id,\n",
    "    pad_token_id=model.config.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Prompts, Inputs and Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing one question at a time, very slow.  \n",
    "Check next cells for batch inferencing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "print_response = True       # To print response for each question\n",
    "\n",
    "for index, row in responses_df.iterrows():\n",
    "    print(f'Started with video ID: {video_id} | Index: {index}')\n",
    "    video_id = row['Video ID']\n",
    "    transcript = row['Transcript']\n",
    "\n",
    "    for question_num in range(1, len(QUESTIONS) + 1):\n",
    "        column_name = f'Response_{question_num}'\n",
    "        if row[column_name] == '':\n",
    "            instruction = ' '.join([QUESTION_HEAD, QUESTIONS[question_num - 1], QUESTION_TAIL])\n",
    "            prompt = prompt_template.format(instruction=instruction, transcript=transcript)\n",
    "\n",
    "            input = tokenizer(prompt, return_tensors='pt', return_attention_mask=False)\n",
    "            input = input.to(device)\n",
    "\n",
    "            output_token_ids = model.generate(**input, generation_config=generation_config)\n",
    "            # llm_output = run_llm(pipe)\n",
    "            llm_response = tokenizer.batch_decode(output_token_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "            check_and_store_response(llm_response, responses_df, video_id, question_num, remove_prompt=True, print_response=print_response)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferencing `batch_size` questions in each transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token   # needed for batch inference\n",
    "tokenizer.padding_side = 'left'             # decoder-only models require left padding_side\n",
    "\n",
    "# start_video_idx = 0\n",
    "batch_size = 15             # Adjust as needed. Max 15 (as the number of questions)\n",
    "print_response = True       # To print response for each question\n",
    "start_video_idx = find_first_empty_string_row_index(responses_df)\n",
    "\n",
    "def generate_prompt(question_num, transcript):\n",
    "    instruction = ' '.join([QUESTION_HEAD, QUESTIONS[question_num - 1], QUESTION_TAIL])\n",
    "    return prompt_template.format(instruction=instruction, transcript=transcript)\n",
    "\n",
    "def generate_responses(prompts):\n",
    "    encoded_batch_inputs = tokenizer(prompts, return_tensors='pt', padding=True,).to(device)\n",
    "\n",
    "    # Generate responses for the batch\n",
    "    with torch.no_grad():   # Disable gradients for inference\n",
    "        outputs_token_ids = model.generate(**encoded_batch_inputs, generation_config=generation_config)\n",
    "    del encoded_batch_inputs                # to free memory\n",
    "\n",
    "    decoded_responses = tokenizer.batch_decode(outputs_token_ids, skip_special_tokens=True)\n",
    "    del outputs_token_ids                   # to free memory\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return decoded_responses\n",
    "\n",
    "def store_responses(responses, responses_df, video_id, question_num):\n",
    "    for i, response in enumerate(responses, start=question_num - len(responses) + 1):\n",
    "        check_and_store_response(response, responses_df, video_id, i, remove_prompt=True, print_response=print_response)\n",
    "\n",
    "def process_questions_and_responses(responses_df, start_video_idx, batch_size):\n",
    "    if start_video_idx is None:\n",
    "        return\n",
    "\n",
    "    video_ids = responses_df['Video ID'][start_video_idx:]\n",
    "    transcripts = responses_df['Transcript'][start_video_idx:]\n",
    "\n",
    "    for video_index, (video_id, transcript) in enumerate(zip(video_ids, transcripts), start=start_video_idx):\n",
    "        print('\\nStarted with video: ', video_id, ' |  Index: ', video_index)\n",
    "\n",
    "        prompts_batch = [] \n",
    "        # Iterate over questions and create prompts\n",
    "        for question_num in range(1, len(QUESTIONS) + 1):\n",
    "            prompt = generate_prompt(question_num, transcript)\n",
    "            prompts_batch.append(prompt)\n",
    "\n",
    "            # Generate responses for prompts in batches\n",
    "            if len(prompts_batch) == batch_size or question_num == len(QUESTIONS):\n",
    "                responses_batch = generate_responses(prompts_batch)\n",
    "                \n",
    "                store_responses(responses_batch, responses_df, video_id, question_num)\n",
    "                del responses_batch         # to free memory\n",
    "                prompts_batch.clear()       # Clear prompts after processing in batch\n",
    "\n",
    "%time process_questions_and_responses(responses_df, start_video_idx, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch `n` number of transcripts (`n * 15` prompts) at a time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch \"n\" number of transcripts at a time\n",
    "\"\"\"\n",
    "tokenizer.pad_token = tokenizer.eos_token   # needed for batch inference\n",
    "tokenizer.padding_side = 'left'             # decoder only models require left padding_side\n",
    "\n",
    "start_video_idx = find_first_empty_string_row_index(responses_df)\n",
    "n = 2                       # Number of transcripts\n",
    "batch_size = n * 15         # or smaller\n",
    "print_response = True       # To print response for each question\n",
    "\n",
    "def generate_prompts(transcripts):\n",
    "    prompts = []\n",
    "    for transcript in transcripts:\n",
    "        for question in QUESTIONS:\n",
    "            instruction = ' '.join([QUESTION_HEAD, question, QUESTION_TAIL])\n",
    "            prompt = prompt_template.format(instruction=instruction, transcript=transcript)\n",
    "            prompts.append(prompt)\n",
    "    return prompts\n",
    "\n",
    "def generate_responses(prompts, batch_size):\n",
    "    responses = []\n",
    "    for i in range(0, len(prompts), batch_size):\n",
    "        batch_prompts = prompts[i:i + batch_size]\n",
    "        encoded_batch_inputs = tokenizer(batch_prompts, return_tensors='pt', padding=True,).to(device)\n",
    "\n",
    "        with torch.no_grad():  # Disable gradients for inference\n",
    "            outputs_token_ids = model.generate(**encoded_batch_inputs, generation_config=generation_config)\n",
    "        del encoded_batch_inputs\n",
    "\n",
    "        decoded_responses = tokenizer.batch_decode(outputs_token_ids, skip_special_tokens=True)\n",
    "        del outputs_token_ids\n",
    "\n",
    "        responses.extend(decoded_responses)\n",
    "        del decoded_responses\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    return responses\n",
    "\n",
    "def store_responses(responses, video_ids_batch, responses_df, batch_start_index=0):\n",
    "    for i, response in enumerate(responses):\n",
    "        video_index = (batch_start_index + i) % len(video_ids_batch)    # Determine the index of the video for this prompt\n",
    "        video_id = video_ids_batch[video_index]     # Get the video ID corresponding to the video index\n",
    "        question_num = (i) % len(QUESTIONS) + 1     # Calculate the question number based on the current iteration\n",
    "\n",
    "        check_and_store_response(response, responses_df, video_id, question_num, remove_prompt=True, print_response=print_response)\n",
    "\n",
    "def generate_outputs(responses_df, start_video_idx, batch_size, n):\n",
    "    if start_video_idx is None:\n",
    "        return\n",
    "\n",
    "    video_ids = responses_df['Video ID'].values[start_video_idx:]\n",
    "    transcripts = responses_df['Transcript'].values[start_video_idx:]\n",
    "    num_videos = len(responses_df)\n",
    "\n",
    "    for video_index in range(start_video_idx, num_videos, n):\n",
    "        video_ids_batch = video_ids[video_index : min(video_index + n, num_videos)]\n",
    "        transcripts_batch = transcripts[video_index : min(video_index + n, num_videos)]\n",
    "\n",
    "        print(f\"\\nStarted with video(s): {', '.join(video_ids_batch)} | \"\n",
    "            f'Index: {video_index} to {min(video_index + n, num_videos) - 1}')\n",
    "\n",
    "        prompts_batch = generate_prompts(transcripts_batch)\n",
    "\n",
    "        responses_batch = generate_responses(prompts_batch, batch_size)\n",
    "        \n",
    "        store_responses(responses_batch, video_ids_batch, responses_df, video_index)\n",
    "\n",
    "%time generate_outputs(responses_df, start_video_idx, batch_size, n)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_from_index = start_video_idx\n",
    "index_of_q1 = responses_df.columns.get_loc(\"Q1\")\n",
    "\n",
    "responses_df.iloc[display_from_index:, index_of_q1:index_of_q1+15].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_none = (responses_df.isna() | (responses_df == '')).sum()\n",
    "columns_with_none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_with_none = responses_df[responses_df.isna().any(axis=1)]\n",
    "rows_with_none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_with_problems = responses_df[responses_df['Problem'].apply(lambda x: len(x) > 0)].index.tolist()\n",
    "print(indices_with_problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "if indices_with_problems:\n",
    "    index_with_problem = 7\n",
    "    responses_with_problem_list = list(responses_df.loc[index_with_problem, 'Problem'])\n",
    "    print(\"List of questions with problem:\", responses_with_problem_list)\n",
    "\n",
    "    response_with_problem = responses_with_problem_list[0]\n",
    "    text = responses_df.loc[index_with_problem, f'Response_{response_with_problem}']\n",
    "    display(HTML(\"<div style='white-space: pre-wrap;'>{}</div>\".format(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the full responses for a specific transcript\n",
    "index_to_display = 0\n",
    "for question_num in range(1, 16):\n",
    "    print(f'Q{question_num}:\\n', responses_df.at[index_to_display,f'Response_{question_num}'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Results in a CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_output_file = os.path.join(responses_dir, f'{results_file_name}-response.csv')\n",
    "\n",
    "responses_df.to_csv(csv_output_file, index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
