{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from openai import OpenAI\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')  # Add the parent directory of LLM_Evaluations to the Python path\n",
    "from Utils.llm_evaluation_utils import load_responses_df, \\\n",
    "                        check_and_store_response,   \\\n",
    "                        build_question_prompt,      \\\n",
    "                        QUESTION_SETS\n",
    "\n",
    "api_key = os.environ.get('OPENAI_API_KEY')\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "model_name = 'gpt-4o'\n",
    "max_tokens = 200\n",
    "temperature = 1\n",
    "question_type = 'ZS'\n",
    "QUESTIONS = QUESTION_SETS[question_type]['QUESTIONS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defined-Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_timestamp(timestamp):\n",
    "    '''Converts a Unix timestamp to a datetime object.'''\n",
    "    return datetime.fromtimestamp(timestamp) if timestamp is not None else None\n",
    "\n",
    "def retrieve_file_name(file_id, client: OpenAI):\n",
    "    '''Retrieve the filename associated with the given file ID.'''\n",
    "    try:\n",
    "        return client.files.retrieve(file_id).filename\n",
    "    except Exception as e:\n",
    "        print(f'Error retrieving file ID {file_id}:', e)\n",
    "        return ''\n",
    "\n",
    "def upload_file(file_path, client: OpenAI):\n",
    "    '''Upload a JSONL file and return the file ID from OpenAI's server.'''\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            file_upload_response = client.files.create(\n",
    "                file=file,\n",
    "                purpose='batch'\n",
    "            )\n",
    "        return file_upload_response\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def create_batch(input_file_id, client: OpenAI):\n",
    "    '''Create a batch request and return the response.'''\n",
    "    try:\n",
    "        batch_response = client.batches.create(\n",
    "            input_file_id=input_file_id,\n",
    "            endpoint='/v1/chat/completions',\n",
    "            completion_window='24h'\n",
    "        )\n",
    "        return batch_response\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "def check_batch(batch_id, client: OpenAI):\n",
    "    '''Retrieve batch information using the provided batch ID.'''\n",
    "    try:\n",
    "        return client.batches.retrieve(batch_id)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def cancel_batch(batch_id, client: OpenAI):\n",
    "    '''Cancel a batch with the provided batch ID and return the cancellation response.'''\n",
    "    try:\n",
    "        return client.batches.cancel(batch_id)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "def extract_batch_num(filename):\n",
    "    '''\n",
    "    Extract the batch number from the filename. \n",
    "    Filename should ends with an integer before the extension.\n",
    "    Example: `prompts-batch_3.jsonl`\n",
    "    '''\n",
    "    match = re.search(r'(\\d+)\\.', filename)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "def prepare_response_file(input_file_name, response_files_dir, responses_file_name='responses.jsonl'):\n",
    "    '''Generate a response file name based on the batch number.'''\n",
    "    batch_num = extract_batch_num(input_file_name)\n",
    "    if batch_num is not None:\n",
    "        responses_file_name = f\"{responses_file_name.split('.')[0]}-batch_{batch_num}.jsonl\"\n",
    "    responses_file_path = os.path.join(response_files_dir, responses_file_name)\n",
    "    return responses_file_path, responses_file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcripts_dir = '../../Getting_Transcripts'\n",
    "# transcripts_file_name = 'merged_filtered_videos_transcripts.csv'\n",
    "# responses_dir = '../LLMs_Responses'\n",
    "# topics_to_include = ['Spina Bifida', 'Flat Feet', 'Cluster Headache', 'Trigger Finger', 'Pudendal Nerve']\n",
    "\n",
    "transcripts_dir = '../../../ISA_Paper/Data'\n",
    "transcripts_file_name = 'diabetes_videos_transcripts.csv'\n",
    "responses_dir = '../../../ISA_Paper/Data/Results'\n",
    "topics_to_include = ['Insulin Self-Administration']\n",
    "\n",
    "prompt_type = 'GS_prompting'\n",
    "topics = 'diabetes'\n",
    "results_file_name = f'{model_name}-{topics}-{prompt_type}'\n",
    "\n",
    "responses_df = load_responses_df(transcripts_dir, transcripts_file_name, responses_dir, results_file_name, question_type)\n",
    "\n",
    "print('responses_df shape:', responses_df.shape)\n",
    "responses_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Topic' not in responses_df.columns:\n",
    "    experts_file = '../../../Videos_and_DISCERN_data/filtered_experts_scores.csv'\n",
    "    experts_df = pd.read_csv(experts_file)\n",
    "\n",
    "    responses_df = responses_df.merge(experts_df[['Video ID', 'Topic']], on='Video ID', how='left')\n",
    "    responses_df.insert(2, 'Topic', responses_df.pop('Topic'))\n",
    "    responses_df = responses_df[responses_df['Topic'].isin(topics_to_include)]\n",
    "    responses_df = responses_df.reset_index(drop=True)\n",
    "\n",
    "print('responses_df shape:', responses_df.shape)\n",
    "responses_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Build Prompts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_csv_file = './prompts.csv'     # Prompts file to create\n",
    "\n",
    "data = []\n",
    "\n",
    "for _, row in responses_df.iterrows():\n",
    "    for question_num in range(1, len(QUESTIONS) + 1):\n",
    "        if row[f'Q{question_num}'] is None:\n",
    "            video_id = row['Video ID']\n",
    "            transcript = row['Transcript']\n",
    "\n",
    "            custom_id = f'{video_id}&{question_num}'\n",
    "            prompt = build_question_prompt(transcript, question_num, question_type)\n",
    "            data.append([custom_id, prompt])\n",
    "\n",
    "df = pd.DataFrame(data, columns=['ID', 'Prompt'])\n",
    "df.to_csv(prompts_csv_file, index=False, quoting=csv.QUOTE_MINIMAL, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Batch Prompt Requests**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Prepare JSONL File\n",
    "To use OpenAI's Batch API, prompts must be provided in a JSONL (JSON Lines) format.  \n",
    "This script converts CSV to `JSONL` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts_csv_file = './prompts.csv'\n",
    "prompts_jsonl_file = 'Batch_Files/prompts.jsonl'\n",
    "\n",
    "def csv_to_jsonl(csv_file, jsonl_file, model, max_tokens=4096, temperature=1, system='You are a helpful assistant.'):\n",
    "    '''Convert a CSV file to JSON Lines (JSONL) for batch requests using OpenAI API.'''\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(jsonl_file), exist_ok=True)\n",
    "\n",
    "    with open(csv_file, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        with open(jsonl_file, 'w', encoding='utf-8') as jsonlfile:\n",
    "            for row in reader:\n",
    "                custom_id = row['ID']\n",
    "                user_message = row['Prompt']\n",
    "                data = construct_json_line(custom_id, model, user_message, system, max_tokens, temperature)\n",
    "                jsonlfile.write(json.dumps(data) + '\\n')\n",
    "\n",
    "def construct_json_line(custom_id, model, user_message, system_message, max_tokens, temperature):\n",
    "    '''Construct a JSON line for a chat completion request.'''\n",
    "    return {\n",
    "        'custom_id': custom_id, \n",
    "        'method': 'POST', \n",
    "        'url': '/v1/chat/completions', \n",
    "        'body': {\n",
    "            'model': model, \n",
    "            'messages': [\n",
    "                # {'role': 'system', 'content': system_message},\n",
    "                {'role': 'user', 'content': user_message}\n",
    "            ],\n",
    "            'max_tokens': max_tokens,\n",
    "            'temperature': temperature,\n",
    "        }\n",
    "    }\n",
    "\n",
    "csv_to_jsonl(prompts_csv_file, prompts_jsonl_file, model_name, max_tokens, temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since prompts in the JSONL file exceeds the maximum limit of tokens per day (900,000 TPD for tier 1), they have been splitted into multiple JSONL files, and then batched sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_per_file = 70    # Adjust as needed\n",
    "prompts_jsonl_file = 'Batch_Files/prompts.jsonl'\n",
    "splitted_prompts_dir = './Batch_Files/Batch_Prompts'\n",
    "\n",
    "def split_file(input_file_path, output_dir, max_lines_per_file):\n",
    "    '''Split lines into multiple files with a specified maximum number of lines per file.'''\n",
    "    os.makedirs(output_dir, exist_ok=True)        # Ensure the output directory exists, create if not \n",
    "    with open(input_file_path, 'r') as infile:\n",
    "        lines = infile.readlines()\n",
    "\n",
    "    # Calculate the number of output files needed\n",
    "    num_files = (len(lines) + max_lines_per_file - 1) // max_lines_per_file\n",
    "\n",
    "    # Write lines to each output file\n",
    "    for i in range(num_files):\n",
    "        start_index = i * max_lines_per_file\n",
    "        end_index = min((i + 1) * max_lines_per_file, len(lines))\n",
    "        output_filename = f'prompts-batch_{i+1}.jsonl'\n",
    "        with open(os.path.join(output_dir, output_filename), 'w') as outfile:\n",
    "            outfile.writelines(lines[start_index:end_index])\n",
    "\n",
    "split_file(prompts_jsonl_file, splitted_prompts_dir, lines_per_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Upload JSONL File(s) to OpenAI Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_prompts_dir = './Batch_Files/Batch_Prompts'           # The directory containing the JSONL file(s)\n",
    "\n",
    "# Get the list of JSONL files in the directory\n",
    "jsonl_files = [file for file in os.listdir(batch_prompts_dir) if file.endswith('.jsonl')]\n",
    "\n",
    "# Upload each JSONL file\n",
    "for prompts_batch_file in jsonl_files:\n",
    "    jsonl_batch_file_path = os.path.join(batch_prompts_dir, prompts_batch_file)\n",
    "    file_upload_response = upload_file(jsonl_batch_file_path, client)\n",
    "    if file_upload_response:\n",
    "        input_file_id = file_upload_response.id\n",
    "        print(f'File ID for {prompts_batch_file}: {input_file_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_and_sort_file_ids(text):\n",
    "#     pattern = re.compile(r'File ID for (prompts-batch_\\d+\\.jsonl): (\\S+)')\n",
    "#     matches = pattern.findall(text)\n",
    "#     sorted_matches = sorted(matches, key=lambda x: int(re.search(r'\\d+', x[0]).group()))\n",
    "#     return dict(sorted_matches)\n",
    "\n",
    "# files_list = '''\n",
    "# File ID for prompts-batch_9.jsonl: file-80P02rE5rTRSF0aMAvfz2khh\n",
    "# '''\n",
    "# batch_files_dict = extract_and_sort_file_ids(files_list)\n",
    "# batch_files_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display List of Files in OpenAI Account\n",
    "This includes uploaded files, as well as output files generated by OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    list_files = client.files.list(\n",
    "        # purpose='batch'       # Specify purpose of file (batch, fine-tuning, assistant, etc).\n",
    "    )\n",
    "    for i in range(len(list_files.data)):\n",
    "        file_data = list_files.data[i]\n",
    "        print(f'File: {file_data.id} | Purpose: {file_data.purpose:<12} | ', end='')\n",
    "        print(f'Created at: {convert_timestamp(file_data.created_at)} | File Name: {file_data.filename}')\n",
    "        \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete a File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_file_id = 'file-BMkcaccsiJqcxpwQjXCmECcr'\n",
    "\n",
    "try:\n",
    "    response = client.files.delete(delete_file_id)\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(e) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create a Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_id = 'file-GIai7j5wL4FHARjUNfii7Uml'\n",
    "\n",
    "# Execute the batch creation\n",
    "create_batch_response = create_batch(input_file_id, client)\n",
    "\n",
    "# Display batch information\n",
    "if create_batch_response:\n",
    "    print('Batch status:', create_batch_response.status)\n",
    "    \n",
    "    file_name = retrieve_file_name(input_file_id, client)\n",
    "    if file_name:\n",
    "        print('Batch file name:', file_name)\n",
    "\n",
    "    batch_id = create_batch_response.id\n",
    "    print('Batch ID:', batch_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Monitor and Retrieve Batch Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_id = 'batch_rVzH8duBdVA9tpMQjstW2d19'\n",
    "\n",
    "def print_label_and_timestamp(label, timestamp):\n",
    "    '''Prints the provided label and timestamp in a human-readable format.'''\n",
    "    print(label + ':', convert_timestamp(timestamp)) if timestamp is not None else None\n",
    "\n",
    "# Retrieve batch information\n",
    "check_batch_response = check_batch(batch_id, client)\n",
    "\n",
    "# Display batch information\n",
    "if check_batch_response:\n",
    "    print('Batch status:', check_batch_response.status)\n",
    "    request_counts = check_batch_response.request_counts\n",
    "    print(f'Completed: {request_counts.completed:<6}| Failed: {request_counts.failed:<6}| Total: {request_counts.total:<6}\\n')\n",
    "\n",
    "    if check_batch_response.errors:\n",
    "        print('Batch error:', check_batch_response.errors)\n",
    "    else:\n",
    "        event_names = ['created_at', 'expires_at', 'completed_at', 'expired_at', 'failed_at', 'cancelled_at']\n",
    "        for event_name in event_names:\n",
    "            print_label_and_timestamp(f\"Batch {event_name.replace('_', ' ')}\", getattr(check_batch_response, event_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display List of Batch Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    list_batches = client.batches.list(\n",
    "        limit=10    # Number of batches to display\n",
    "    )\n",
    "    for i in range(len(list_batches.data)):\n",
    "        batch_data = list_batches.data[i]\n",
    "        input_file_name = retrieve_file_name(batch_data.input_file_id, client)\n",
    "        print(f'Batch: {batch_data.id} | Status: {batch_data.status:<11} ', end='')\n",
    "        print(f'| Created at: {convert_timestamp(batch_data.created_at)} | Input File: {input_file_name}')\n",
    "        \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <u>Cancel</u> a Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_id = 'batch_wHUxDnkfhvQf3JXlHAakGD20'\n",
    "\n",
    "cancel_batch_response = cancel_batch(batch_id, client)\n",
    "\n",
    "if cancel_batch_response:\n",
    "    print('Batch status:', cancel_batch_response.status)\n",
    "    if cancel_batch_response.errors:\n",
    "        print('Batch error:', cancel_batch_response.errors)\n",
    "    else:\n",
    "        if cancel_batch_response.cancelled_at is not None:\n",
    "            print('Batch canceled at:', convert_timestamp(cancel_batch_response.cancelled_at))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Retrieve Response File Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_files_dir = './Batch_Files/Batch_Responses'         # Directory where the JSONL file(s) wil be stored\n",
    "responses_file_name = 'responses.jsonl'                      # Desired name of the responses file\n",
    "batch_id = 'batch_rVzH8duBdVA9tpMQjstW2d19'\n",
    "\n",
    "# Retrieve batch information\n",
    "check_batch_response = check_batch(batch_id, client)\n",
    "\n",
    "if check_batch_response:\n",
    "    print('Batch status:', check_batch_response.status)\n",
    "\n",
    "    input_file_name = retrieve_file_name(check_batch_response.input_file_id, client)\n",
    "    if input_file_name:\n",
    "        print('Related to input file:', input_file_name, '\\n')\n",
    "    \n",
    "    if check_batch_response.status == 'completed' and check_batch_response.output_file_id:\n",
    "        responses_file_path, responses_file_name =  prepare_response_file(input_file_name, \n",
    "                                               response_files_dir, \n",
    "                                               responses_file_name)\n",
    "\n",
    "        output_file_id = check_batch_response.output_file_id\n",
    "        print('Output file ID:', output_file_id)\n",
    "\n",
    "        # Retrieve content of the output file\n",
    "        batch_output_content = client.files.content(output_file_id)\n",
    "\n",
    "        # Write output file content to disk\n",
    "        batch_output_content.write_to_file(responses_file_path)\n",
    "        print('Content of batch response retrieved successfully:', responses_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatively, Steps 2-5: Batching sequentially automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_from_batch = 1        # Batch number to start from\n",
    "batch_prompts_dir = './Batch_Files/Batch_Prompts'       # The directory containing the JSONL file(s)\n",
    "response_files_dir = './Batch_Files/Batch_Responses'    # Directory where the output JSONL file(s) wil be stored\n",
    "\n",
    "def create_output_directory(response_files_dir):\n",
    "    '''Create the output directory if it doesn't exist.'''\n",
    "    os.makedirs(response_files_dir, exist_ok=True)\n",
    "\n",
    "def filter_files_by_batch(jsonl_files, start_from_batch):\n",
    "    '''Filter JSONL files based on the starting batch number.'''\n",
    "    filtered_files = [file for file in jsonl_files if extract_batch_num(file) is not None]\n",
    "    filtered_files = sorted(filtered_files, key=extract_batch_num)\n",
    "    return [file for file in filtered_files if extract_batch_num(file) >= start_from_batch]\n",
    "\n",
    "def process_batch_file(batch_prompts_dir, prompts_batch_file, client: OpenAI):\n",
    "    '''Process a single JSONL batch file.'''\n",
    "    jsonl_batch_file_path = os.path.join(batch_prompts_dir, prompts_batch_file)\n",
    "    file_upload_response = upload_file(jsonl_batch_file_path, client)\n",
    "    if file_upload_response:\n",
    "        input_file_id = file_upload_response.id\n",
    "        print(f'File ID for {prompts_batch_file}: {input_file_id}')\n",
    "        return input_file_id\n",
    "    return None\n",
    "\n",
    "def create_batch_and_wait(input_file_id, prompts_batch_file, client: OpenAI):\n",
    "    '''Create a batch and wait for completion.'''\n",
    "    create_batch_response = create_batch(input_file_id, client)\n",
    "    if create_batch_response:\n",
    "        batch_id = create_batch_response.id\n",
    "        print(f'Batch for {prompts_batch_file} created with ID: {batch_id}')\n",
    "    else:\n",
    "        print(f'Batch for {prompts_batch_file} not created')\n",
    "        return None\n",
    "\n",
    "    while True:\n",
    "        check_batch_response = check_batch(batch_id, client)\n",
    "        if not check_batch_response or check_batch_response.status == 'failed':\n",
    "            print(f'Batch {batch_id} failed\\n')\n",
    "            return None\n",
    "            \n",
    "        if check_batch_response.status == 'completed' and check_batch_response.output_file_id:\n",
    "            print(f'######## Batch {prompts_batch_file} has completed. ########')\n",
    "            return check_batch_response.output_file_id\n",
    "        else:\n",
    "            time.sleep(2 * 60)  # Delay for 2 minutes\n",
    "\n",
    "def retrieve_and_save_response(output_file_id, response_files_dir, prompts_batch_file, client: OpenAI):\n",
    "    '''Retrieve and save the response content.'''\n",
    "    responses_file_path, responses_file_name = prepare_response_file(prompts_batch_file, response_files_dir)\n",
    "    batch_output_content = client.files.content(output_file_id)     \n",
    "    batch_output_content.write_to_file(responses_file_path)        \n",
    "    print('Content of batch response retrieved successfully:', responses_file_name, '\\n')\n",
    "\n",
    "def process_batches(batch_prompts_dir, response_files_dir, client: OpenAI, start_from_batch):\n",
    "    create_output_directory(response_files_dir)\n",
    "\n",
    "    jsonl_files = [file for file in os.listdir(batch_prompts_dir) if file.endswith('.jsonl')]\n",
    "    filtered_files = filter_files_by_batch(jsonl_files, start_from_batch)\n",
    "\n",
    "    for prompts_batch_file in filtered_files:\n",
    "        input_file_id = process_batch_file(batch_prompts_dir, prompts_batch_file, client)\n",
    "        if input_file_id:\n",
    "            output_file_id = create_batch_and_wait(input_file_id, prompts_batch_file, client)\n",
    "            if output_file_id:\n",
    "                retrieve_and_save_response(output_file_id, response_files_dir, prompts_batch_file, client)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "process_batches(batch_prompts_dir, response_files_dir, client, start_from_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine all jsonl files into a single file, if splitted and sent in multiple batches earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_responses_dir = './Batch_Files/Batch_Responses'   # Directory containing the JSONL files\n",
    "first_response_file = 'responses-batch_1.jsonl'                  # First file in the sequence (e.g., 'responses-batch_1.jsonl')\n",
    "                                                        # Expected sequence format: 'responses-batch_{number}.jsonl'\n",
    "responses_jsonl_file = './Batch_Files/responses.jsonl'           # Desired output JSONL file path\n",
    "\n",
    "def extract_prefix_before_last_num(text):\n",
    "    '''Extracts the prefix before the last number in the given text.'''\n",
    "    integers = re.findall(r'\\d+', text)             # Find all integers in the string\n",
    "    if not integers:\n",
    "        raise ValueError('No number found in the input text.')\n",
    "    last_integer = int(integers[-1])                # Extract the last number\n",
    "    return text.rsplit(str(last_integer), 1)[0]     # Extract the prefix before the last number\n",
    "\n",
    "def combine_jsonl_files(input_dir, output_file, first_file):\n",
    "    '''Combine JSONL files in the input directory and write to the output JSONL file.'''\n",
    "    if not first_file:\n",
    "        raise ValueError(\"The 'first_file' argument must be provided to determine the prefix.\")\n",
    "    file_prefix = extract_prefix_before_last_num(first_file)\n",
    "    if not os.path.isdir(input_dir):\n",
    "        raise FileNotFoundError(f\"Input directory '{input_dir}' does not exist.\")\n",
    "    if not os.listdir(input_dir):\n",
    "        raise FileNotFoundError(f\"Input directory '{input_dir}' is empty.\")\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as out_jsonl:\n",
    "        for file_name in os.listdir(input_dir):\n",
    "            if file_name.startswith(file_prefix) and file_name.endswith('.jsonl'):\n",
    "                file_path = os.path.join(input_dir, file_name)\n",
    "                with open(file_path, 'r', encoding='utf-8') as in_jsonl:\n",
    "                    for line in in_jsonl:\n",
    "                        out_jsonl.write(line)\n",
    "    print('Responses are now combined in one JSONL file.')\n",
    "\n",
    "combine_jsonl_files(batch_responses_dir, responses_jsonl_file, first_response_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Extracting Results From Repsonses**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract assistant messages to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_jsonl_file = './Batch_Files/responses.jsonl'\n",
    "\n",
    "def get_video_id_and_question_num(custom_id):\n",
    "    video_id = custom_id.split('&')[0]\n",
    "    question_num = int(custom_id.split('&')[1])\n",
    "    return video_id, question_num\n",
    "\n",
    "# Read data from the output JSONL file and populate the dictionary\n",
    "with open(responses_jsonl_file, 'r', encoding='utf-8') as jsonlfile:\n",
    "    for line in jsonlfile:\n",
    "        data = json.loads(line.strip())\n",
    "        custom_id = data.get('custom_id', '')\n",
    "        response = data.get('response', {})\n",
    "        \n",
    "        if response.get('status_code') == 200:\n",
    "            response_body = response.get('body', {})\n",
    "            if response_body:\n",
    "                assistant_response = response_body.get('choices', [])[0].get('message', {}).get('content', '')\n",
    "                video_id, question_num = get_video_id_and_question_num(custom_id)\n",
    "                check_and_store_response(assistant_response, responses_df, video_id, question_num, rating_scale=5, )\n",
    "            else:\n",
    "                print(f'No response body found for custom ID: {custom_id}')\n",
    "        else:\n",
    "            print(f\"Error for custom ID: {custom_id}, status_code: {response.get('status_code')}\")\n",
    "\n",
    "responses_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_none = (responses_df.isna() | (responses_df == '')).sum()\n",
    "columns_with_none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_with_none = responses_df[responses_df.isna().any(axis=1)]\n",
    "rows_with_none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_with_problems = responses_df[responses_df['Problem'].apply(lambda x: len(x) > 0)].index.tolist()\n",
    "print(indices_with_problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "if indices_with_problems:\n",
    "    index_with_problem = 54\n",
    "    responses_with_problem_list = list(responses_df.loc[index_with_problem, 'Problem'])\n",
    "    print(\"List of questions with problem:\", responses_with_problem_list)\n",
    "\n",
    "    response_with_problem = responses_with_problem_list[0]\n",
    "    text = responses_df.loc[index_with_problem, f'Response_{response_with_problem}']\n",
    "    display(HTML(\"<div style='white-space: pre-wrap;'>{}</div>\".format(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the full responses for a specific transcript\n",
    "index_to_display = 25\n",
    "for question_num in range(1, 16):\n",
    "    print(f'Q{question_num}:', responses_df.at[index_to_display,f'Response_{question_num}'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Results in a CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_output_file = os.path.join(responses_dir, f'{results_file_name}-response.csv')\n",
    "\n",
    "responses_df.to_csv(csv_output_file, index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
